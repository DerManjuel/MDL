{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerManjuel/MDL/blob/main/MDL_Exercise4_Explainability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdI9yHAsad0U",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Medical Deep Learning\n",
        "## Exercise 4: Weakly-Supervised Visualization\n",
        "\n",
        "The goal of this exercise is to implement methods that allow to gain insights which parts of an input image to a Deep Neural Network are pivotal for its classification decision. Because of their conception as black-box-system, especially in Medical Imaging this is important for user acceptance.\n",
        "\n",
        "![explainability](https://drive.google.com/uc?export=view&id=1oDUfMmOpopV8Dnv4rvHmqmxMcIK8JLYI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yc8pTrz4ad0W",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "TuNoWj4mUXmD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import matplotlib.cm as mpl_color_map\n",
        "import os\n",
        "import wget\n",
        "import zipfile\n",
        "from tqdm.notebook import trange, tqdm\n",
        "\n",
        "%matplotlib inline\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgXbZICnad0b",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "dataset_url = 'https://cloud.imi.uni-luebeck.de/s/KwD3fs4q6ctqmA8/download'\n",
        "\n",
        "def get_data(data_url):\n",
        "    filename = './MDL3_data.zip'\n",
        "    if not os.path.exists(filename):\n",
        "        filename = wget.download(data_url)\n",
        "    zipfile.ZipFile(filename,'r').extractall()\n",
        "    \n",
        "get_data(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9-jKcCNad0f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Function for Heatmap Overlays (expects gray-scale image as numpy and heatmap as torch array)\n",
        "def overlayParula(grayim_numpy: np.ndarray, heatmap_torch: torch.Tensor, smooth : bool=False):\n",
        "    \n",
        "    heat_max = torch.kthvalue(heatmap_torch.view(-1), int(.95*heatmap_torch.numel()))[0]\n",
        "    heat_min = torch.min(heatmap_torch)\n",
        "    heatmap_torch = torch.clamp((heatmap_torch-heat_min)/(heat_max-heat_min),0,1)\n",
        "    if(smooth==True):\n",
        "        heatmap_torch = F.avg_pool2d(F.avg_pool2d(F.max_pool2d(heatmap_torch.unsqueeze(0).unsqueeze(0),3,stride=1,padding=1),3,stride=1,padding=1),3,stride=1,padding=1).squeeze()\n",
        "\n",
        "\n",
        "    x = np.linspace(0.0, 1.0, 256)\n",
        "    rgb_jet = mpl_color_map.get_cmap(plt.get_cmap('jet'))(x)[:,:3]\n",
        "    rgb_gray = mpl_color_map.get_cmap(plt.get_cmap('gray'))(x)[:,:3]\n",
        "    #rgb_viridis\n",
        "    rgb_heat = rgb_jet[(heatmap_torch*255).numpy().astype('uint8'),:]\n",
        "    rgb_base = rgb_gray[(grayim_numpy*255).astype('uint8'),:]\n",
        "\n",
        "    rgb0 = (rgb_heat*127.5+rgb_base*127.5).astype('uint8')\n",
        "    weight = torch.tanh((heatmap_torch-0.5)*3)*0.5+0.5\n",
        "    alpha = torch.clamp(1.0 - 0.5*weight,0,1.0)\n",
        "    overlay = rgb_base*alpha.unsqueeze(2).numpy() + rgb_heat*(1.0-alpha).unsqueeze(2).numpy()\n",
        "        \n",
        "    return overlay\n",
        "\n",
        "# Function to compute the number of trainable parameters in a given model\n",
        "def countParameters(model):\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    return params\n",
        "\n",
        "# Function to compute the Dice value given the ground truth segmentation of a slice\n",
        "# and the guided_backpropagation_map or the CAM map as <pred>\n",
        "def oracle_dice(pred,gt_th):\n",
        "    val,idx = torch.sort(pred.data.flatten(),descending=True)\n",
        "    gt_sort = gt_th.flatten()[idx]\n",
        "    intersect = torch.cumsum(gt_sort,0).float()\n",
        "    cardinal = (gt_th.flatten()>0.5).float().sum()+torch.arange(pred.numel(), device=gt_th.device).float()\n",
        "    all_dices = 2*intersect/torch.clamp(cardinal,min=.001)\n",
        "    return all_dices.max(),val[all_dices.argmax()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYZUwYcLad0g",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Task 0 (15 points): Implement dataset and visualize the given pancreas train & test data\n",
        "We start with implementing the `PancreasDataSet`, which loads the training and test data respectively to its `mode` (`train` or `test`). For training data only the labels are given, for the test datasets also segmentation masks are provided. Because our pretrained CNN on ImageNet expects an RGB-image, we stack three neighboring slices of the CT as feature channels.\n",
        "The training data are given as a list of tensors with shape $[64\\times3\\times224\\times224]$ for the image and $[64]$ for the labels.\n",
        "+ load the data and its labels accordingly to the dataset's mode and concatenate each list to one tensor in dimension $N$ using `torch.cat`\n",
        "+ divide the images with 255 to map them to $[0, 1]$\n",
        "+ if `mode=='test'` load the segmentation masks and concatenate them like above\n",
        "+ z-standardize the images to match the ImageNet value distribution with the provided statistic\n",
        "+ complete the `__len__` and `__getitem__` functions\n",
        "    + **NOTE** the number of returned data differs between the `mode`s. Please use following order for the return statements: images, labels, (segmentations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "m4zwpHiwUXmj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class PancreasDataSet(Dataset):\n",
        "    def __init__(self, mode: str, normalize: bool=True):\n",
        "        \"\"\"\n",
        "        constructor\n",
        "        :param mode: decides whether the training or test split should be loaded\n",
        "        :param normalize: normalize the images with the given statistics\n",
        "        \"\"\"\n",
        "        assert mode in ['train', 'test']\n",
        "        self.mode = mode\n",
        "        super(PancreasDataSet, self).__init__()\n",
        "        \n",
        "        # load data\n",
        "        self.imgs = # todo\n",
        "\n",
        "        # load lables\n",
        "        self.lables = # todo\n",
        "        \n",
        "        if mode == 'test':\n",
        "            # load segmentations\n",
        "            self.segmentations = # todo\n",
        "            assert len(self.imgs) == len(self.lables) == len(self.segmentations)\n",
        "        assert len(self.imgs) == len(self.lables)\n",
        "        \n",
        "        # normalize data\n",
        "        if normalize:\n",
        "            # Below we provide the mean and std values from the ImageNet data\n",
        "            means = torch.Tensor([0.485, 0.456, 0.406])\n",
        "            stds = torch.Tensor([0.229, 0.224, 0.225])\n",
        "            \n",
        "            self.imgs = # todo\n",
        "            \n",
        "    def __getitem__(self, index):\n",
        "        if self.mode == 'train':\n",
        "            # todo\n",
        "        else:\n",
        "            # todo\n",
        "    \n",
        "    def __len__(self):\n",
        "        # todo        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0L3sojPad0j",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Visualize some random input training & test slices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "fCvF_MvoUXmm"
      },
      "outputs": [],
      "source": [
        "ds = PancreasDataSet('test', normalize=False)\n",
        "label = ['no pancreas', 'pancreas']\n",
        "n_plots = 6\n",
        "rnd_idx = torch.randperm(len(ds))[:n_plots]\n",
        "\n",
        "fig, axs = plt.subplots(1, n_plots, figsize=(20, 5))\n",
        "for i, idx in enumerate(rnd_idx):\n",
        "    img, lbl, _ = ds[idx]\n",
        "    axs[i].imshow(img[1], cmap='gray')\n",
        "    axs[i].set_title(label[lbl.item()])\n",
        "    axs[i].set_axis_off()\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1l7IMNSdUXmw"
      },
      "outputs": [],
      "source": [
        "rnd_idx = torch.arange(len(ds))[ds.lables == 1]\n",
        "rnd_idx = rnd_idx[torch.randperm(len(rnd_idx))[:n_plots]]\n",
        "\n",
        "fig, axs = plt.subplots(2, n_plots, figsize=(20, 5))\n",
        "for i, idx in enumerate(rnd_idx):\n",
        "    axs[0, i].imshow(ds.imgs[idx, 1], cmap='gray')\n",
        "    axs[0, i].set_axis_off()\n",
        "    \n",
        "    axs[1, i].imshow(ds.segmentations[idx])\n",
        "    axs[1, i].set_axis_off()\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wot8rM9Bad0n",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Task 1 (25 points): Finetune a pretrained ResNet on the given data\n",
        "Before we can fine tune the pretrained CNN on our data, we have to modify its architecture to our needs.\n",
        "+ load the **pretrained** ResNet18 from the torchvision model zoo\n",
        "+ get familiar with the architecture's structure\n",
        "    + print its layers with their settings using the `print` function\n",
        "    + check the corresponding output shapes with `torchinfo.summary`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "vUZVwbICUXmz"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "# load model\n",
        "model = # todo\n",
        "\n",
        "# inspect architecture\n",
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KLVugYQ1UXm0"
      },
      "source": [
        "With foresight to the gradCAM method, we need the last feature layer to have a sufficiently large spatial dimensions therefore we need to replace the ResNet.Layer4 Sequential block with an identity operation (`nn.Identity`). Since in addition to a larger spatial output of the last feature layer, we also want a more lightweight model, we modify the building blocks of Layer3:\n",
        "\n",
        "1. Replace `conv1` in `layer3[0]` with a Conv2D: (in=128,out=256,k_sz=3, stride=1, pad=1, bias=False)\n",
        "2. Replace `conv2` in `layer3[0]` with a Conv2D: (in=256,out=256,k_sz=3, stride=1, dil=2, pad=2, bias=False)\n",
        "3. Replace `downsample[0]` in `layer3[0]` with a Conv2D: (in=128,out=256,k_sz=1, stride=1, pad=0, bias=False)\n",
        "4. Replace `conv1` in `layer3[1]` with a Conv2D: (in=256,out=256,k_sz=3, stride=1, dil=2, pad=2, bias=False)\n",
        "5. Replace `conv2` in `layer3[1]` with a Conv2D: (in=256,out=256,k_sz=3, stride=1, dil=2, pad=2, bias=False)\n",
        "\n",
        "Finally: replace the fully connected layer: Inspect the number of input & output channels for the ImageNet classification task and modify this layer accordingly to our \"(presence/absence) of the pancreas\" classification problem.\n",
        "\n",
        "Now, we have a ResNet14. Check its architecture using `summary`. You should obtain a total count of $2.783.298$ parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bd5Kppzad0o",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# TODO replace the 4th layer with an identity mapping\n",
        "model.layer4 = # todo\n",
        "\n",
        "# TODO: replace the modules according to the description given above\n",
        "\n",
        "\n",
        "# TODO: replace the fully connected layer\n",
        "model.fc = # todo\n",
        "\n",
        "# TODO: print the summary\n",
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUGD_TSvad0s",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Now let's finetune the ResNet14 to our training data:\n",
        "+ create two dataloader (hint: set `pin_memory=True` for speed up during CPU→GPU transfer)\n",
        "    + choose a batch size of 32 for training and shuffle the dataset after every epoch\n",
        "    + you can double the batch size for testing \n",
        "- use Adam in combination with an exponential learning rate scheduler ($\\gamma=0.95$) and cross entropy\n",
        "- you should easily achieve accuracy scores on the training data of > 0.98 after 3 epochs\n",
        "- your validation accuracy should be ~80% after 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xxMNEInuUXm5"
      },
      "outputs": [],
      "source": [
        "dl_train = # todo\n",
        "dl_test = # todo\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# optimizer\n",
        "optimizer = # todo\n",
        "\n",
        "# learning rate scheduler\n",
        "lr_scheduler = # todo\n",
        "\n",
        "# criterion\n",
        "criterion = # todo\n",
        "\n",
        "# for reproducibility (do not change)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# statistics\n",
        "train_loss = torch.zeros(num_epochs, device='cuda')\n",
        "train_acc = torch.zeros_like(train_loss)\n",
        "\n",
        "test_loss = torch.zeros_like(train_loss)\n",
        "test_acc = torch.zeros_like(train_loss)\n",
        "\n",
        "# for num_epochs\n",
        "for epoch in trange(num_epochs, unit='epoch'):\n",
        "\n",
        "    # train mode\n",
        "    model.train()\n",
        "\n",
        "    # for each mini-batch\n",
        "    for input, target in dl_train:\n",
        "        input = input.cuda()\n",
        "        target = target.cuda()\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        # todo\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output = # todo\n",
        "        loss = # todo\n",
        "\n",
        "        # statistics\n",
        "        train_loss[epoch] += loss.detach()\n",
        "        train_acc[epoch] += torch.mean((torch.argmax(output, dim=-1) == target).float())\n",
        "\n",
        "    # update learning rate\n",
        "    # todo\n",
        "\n",
        "    train_loss[epoch] /= len(dl_train)\n",
        "    train_acc[epoch] /= len(dl_train)\n",
        "\n",
        "    # output\n",
        "    tqdm.write('Epoch {} (train) -- loss: {:.4f} accuracy: {:.4f}'.format(epoch, train_loss[epoch].item(), train_acc[epoch].item()))\n",
        "\n",
        "    # validate\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # eval mode\n",
        "        model.eval()\n",
        "\n",
        "        # for each mini-batch\n",
        "        for input, target, _ in dl_test:\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "            # forward\n",
        "            output = # todo\n",
        "            loss = # todo\n",
        "\n",
        "            # statistics\n",
        "            test_loss[epoch] += loss.detach()\n",
        "            test_acc[epoch] += torch.mean((torch.argmax(output, dim=-1) == target).float())\n",
        "\n",
        "        test_loss[epoch] /= len(dl_test)\n",
        "        test_acc[epoch] /= len(dl_test)\n",
        "\n",
        "        # output\n",
        "        tqdm.write('Epoch {} (valid) -- loss: {:.4f} accuracy: {:.4f}'.format(epoch, test_loss[epoch].item(), test_acc[epoch].item()))\n",
        "        \n",
        "torch.save(model, 'pancreas_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RH7kGIk9ad0x",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "![cam](https://drive.google.com/uc?export=view&id=1CBjTBKCdTHFi6MDoz0hJ1y-Faat56Krz)\n",
        "## Task 2 (30 points): implement the CAM method\n",
        "Now that we have our trained ResNet14 at hand, we want to visualize the class activation map (CAM).\n",
        "Therefore, we need to implement the following steps:\n",
        "\n",
        "+ Beforehand, the fully connected layer operated on 256-channel images without spatial dimensions only due to the `nn.AdaptivePooling` layer. Now, we want to weight each of the $28\\times28$ spatial features with their 256-channels using the trained weights from the fully connected layer. In order to do so, extract the weights for the class activation map from the fully connected layer's weight (remember that we are only interested in the weights for the 'pancreas presence' class).\n",
        "+ Since we are interested in the last convolutional feature layer output, we want to omit the avg_pool & fc layer. (Hint: use `torch.nn.Sequential(*list(model.children())[:-k])`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1_ZeaiFad0y",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# load the net\n",
        "model = # todo\n",
        "# extract weights for cam\n",
        "cam_weight = # todo\n",
        "# remove the avg and fc layer\n",
        "truncated_model = # todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KHklEpSDUXnA"
      },
      "outputs": [],
      "source": [
        "# provided function for plotting the activation heatmaps and comparing them to the ground truth\n",
        "def plot_activations(img: torch.Tensor, activation_heatmap: torch.Tensor, segmentation: torch.Tensor):\n",
        "    \n",
        "    img = img[1].squeeze().cpu().numpy()\n",
        "    \n",
        "    fig, axs = plt.subplots(1, 2)\n",
        "    axs[0].imshow(overlayParula(img, activation_heatmap.cpu().squeeze(), smooth=True))\n",
        "    axs[0].set_axis_off()\n",
        "    axs[0].set_title('prediction')\n",
        "    \n",
        "    axs[1].imshow(overlayParula(img, segmentation.cpu(), smooth=False))\n",
        "    axs[1].set_axis_off()\n",
        "    axs[1].set_title('ground truth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_86psn8OUXnB"
      },
      "source": [
        "Next, we want to generate our segmentation from the latent space. Therefore\n",
        "+ feed the normalized image through our modified model\n",
        "+ generate the CAM, therefore\n",
        "    + multiply each feature map with its corresponding weight of the fully connected classifier\n",
        "    + sum over all feature maps to create a CAM\n",
        "    + upsample the CAM to the spatial dimension of the input image\n",
        "+ visualize and compare the CAM with the given ground truth using the provided `plot_activations` function.\n",
        "+ Finally, use the `oracle_dice` function to compute the DICE score (~0.6mean) between the ground truth and the CAM-based segmentation - remember: using only weak labels during the training stage!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Lg3rpYGnUXnC"
      },
      "outputs": [],
      "source": [
        "dice_all = []\n",
        "for test_idx in range(256,262,2):\n",
        "    # skip img without pancreas\n",
        "    if(ds.lables[test_idx]==0):\n",
        "        continue\n",
        "    \n",
        "    img_normalized = dl_test.dataset.imgs[test_idx].unsqueeze(0).cuda()\n",
        "    img = ds.imgs[test_idx].cuda()\n",
        "    seg = ds.segmentations[test_idx].cuda()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # feed the test image through the modified net\n",
        "        feature_map = # todo\n",
        "        # generate cam\n",
        "        cam = # todo\n",
        "        # upsample the activation map\n",
        "        # todo\n",
        "    \n",
        "    # visualize the overlay    \n",
        "    # todo\n",
        "\n",
        "    # compute the dice score & append it to dice_all\n",
        "    # todo\n",
        "    \n",
        "print('mean dice:', torch.stack(dice_all).mean().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3JfsZQnad01",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Task 3 (30 points): implement the guided backpropagation\n",
        "(see also: http://blog.qure.ai/notes/deep-learning-visualization-gradient-based-methods)\n",
        "\n",
        "The idea of GuidedBackprop is to supress gradient flow at positions where either of input or incoming gradients were negative.\n",
        "Luckily, implementing this idea in PyTorch is quite comfortable, since it allows to write own layers - and especially also to define the backpropagation step on the Python-level, i.e. without having to code in CUDA explicitly, when ultimate performance is not necessary.\n",
        "\n",
        "1. In order to use our pretrained ResNet14, we need to replace the ReLU functions by customized ReLUs that suppress the negative gradient flows according to the above stated cases.\n",
        "\n",
        "2. Afterward, we need to implement to guided backpropagation steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtyuNjHhad01",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Implementing the custom ReLU-Layer\n",
        "(See https://pytorch.org/docs/stable/notes/extending.html for further examples on how to implement custom functions & layers and http://cs231n.github.io/optimization-2/#backprop to refresh your knowledge on local backpropagation using the chain rule if needed.)\n",
        "\n",
        "The cell below contains a simple example of a modified square function. Instead of computing the derivative of $x^2$ as $\\frac{\\partial}{\\partial x}(x^2) = 2x$, our the customized square function sets this to $4x$ during the backward step. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PmYj22Jad02",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#example for a custom function and its nn-module\n",
        "class SquareFunction(torch.autograd.Function):\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        output = input**2\n",
        "        return output\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input = ctx.saved_tensors\n",
        "        grad_input = grad_output*4.0*input[0].float()\n",
        "        return grad_input\n",
        "    \n",
        "    \n",
        "class SquareLayer(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SquareLayer, self).__init__()\n",
        "   \n",
        "    def forward(self, x):\n",
        "        SqFct = SquareFunction.apply\n",
        "        x = SqFct(x)\n",
        "        return x\n",
        "    \n",
        "    \n",
        "test = torch.arange(4).view(2,2).float()\n",
        "test.requires_grad = True\n",
        "\n",
        "mySQLayer = SquareLayer()\n",
        "out = mySQLayer(test)\n",
        "print('input:', test)\n",
        "print('output:', out)\n",
        "back = torch.ones(2,2)\n",
        "out.backward(back)\n",
        "print('gradient:', test.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMoXfG3tad04",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "![guided backpropagation](https://drive.google.com/uc?export=view&id=1kC5h6E9j2lXVLZw0cvkow4N_IcDi1deY)\n",
        "\n",
        "Based on the customized square function as an example, you now have to implement a custom `GradReluLayer`, that cancels out gradient flow where either the input values or the gradient values at the layers output are negative (see guided backpropagation in figure above).\n",
        "To verify your implementation, also write your own sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhelT1RLad05",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#define custom relu function and its nn-module\n",
        "class GradReluFunction(torch.autograd.Function):\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        # todo\n",
        "        return output\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # todo\n",
        "        return grad_input\n",
        "    \n",
        "    \n",
        "class GradReluLayer(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GradReluLayer, self).__init__()\n",
        "   \n",
        "    def forward(self, x):\n",
        "        # todo\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awamrLl1ad08",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "GReLU = GradReluLayer()\n",
        "\n",
        "t_in1 = torch.Tensor([[-2,1],[2,2]])\n",
        "t_in1.requires_grad = True\n",
        "t_out1 = GReLU(t_in1)\n",
        "\n",
        "t_out1.backward(torch.Tensor([[-1,1],[1,-1]]))\n",
        "\n",
        "print('input:', t_in1)\n",
        "print('output:', t_out1)\n",
        "print('gradient:', t_in1.grad)\n",
        "#should zero all negative gradients\n",
        "\n",
        "# write your own sanity check!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKxo5GV_ad0-",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "#GIVEN FUNCTION to replace all default ReLUs\n",
        "def replaceFunc(model):\n",
        "    if(isinstance(model,nn.Sequential)):\n",
        "        if(len(model)>1):\n",
        "            for i in range(len(model)):\n",
        "                replaceFunc(model[i])\n",
        "    for i,j in model._modules.items():\n",
        "        if(isinstance(j,nn.ReLU)):\n",
        "            model._modules[i] = GradReluLayer()\n",
        "            \n",
        "        if(isinstance(j,nn.Sequential)):\n",
        "            model._modules[i] = replaceFunc(model._modules[i])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQQluzqZad1A",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Having the customized ReLU-layer at hands, we are now able to implement the guided backward propagation scheme.\n",
        "1. Define a function `guided_backprop` that expects\n",
        "    + a test_image of size [1, 3, 224, 224]\n",
        "    + the pretrained ResNet with customized ReLUs\n",
        "    + a target_class\n",
        "    + the output will be the guided gradient image\n",
        "2. as a first step when calling the function, make sure that gradients will be computed for the input image\n",
        "3. forward propagate the image through the pretrained model (in evaluation mode!)\n",
        "4. having constructed the autograd graph, we set the gradients of the model to zero\n",
        "5. next, we backpropagate a one-hot-tensor (for the correct class) through this graph towards the input. Therefore, we set our one-hot-tensor as `gradient` in the `backward` call.\n",
        "6. finally, return the summed along the channels (absolute values) gradient image as output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zybgLAJRad1A",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def guided_backprop(input_img, pretrained_model, target_class):\n",
        "    #APPLY GUIDED BACKPROP!\n",
        "    \n",
        "    # ensure gradient flow up until input\n",
        "    # todo\n",
        "    \n",
        "    #important: set model in evaluation mode!\n",
        "    # todo\n",
        "    \n",
        "    # forward pass\n",
        "    # todo)\n",
        "    \n",
        "    # create one-hot target for backprop\n",
        "    # todo\n",
        "    \n",
        "    # zero_grad\n",
        "    # todo\n",
        "    \n",
        "    # backward pass\n",
        "    # todo\n",
        "    \n",
        "    # sum along abs values on channels\n",
        "    # todo\n",
        "    \n",
        "    return guided_grad \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NInNhDfAad1C",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Plug all parts together and visualize the guided backpropagation map for a test image.\n",
        "Choosing the same test patient as for CAM and once again computing the DICE score, you should achieve values ~0.4+."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHxREbHtad1D",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# load the net\n",
        "model = # todo\n",
        "# replace its relu functions\n",
        "# todo\n",
        "\n",
        "torch.manual_seed(50)\n",
        "dice_all = []\n",
        "\n",
        "for test_idx in range(256,262,2):\n",
        "    # skip img without pancreas\n",
        "    if(ds.lables[test_idx]==0):\n",
        "        continue\n",
        "    \n",
        "    img_normalized = dl_test.dataset.imgs[test_idx].unsqueeze(0).cuda()\n",
        "    img = ds.imgs[test_idx].cuda()\n",
        "    seg = ds.segmentations[test_idx].cuda()\n",
        "\n",
        "    # call the guided backprop funktion\n",
        "    # todo\n",
        "\n",
        "    # visualize the map\n",
        "    # todo\n",
        "\n",
        "    # compute the dice value & append it to dice_all\n",
        "    # todo\n",
        "\n",
        "print('mean dice:',torch.mean(torch.stack(dice_all)).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus task: Explainable pneumonia classification (10 bonus points)\n",
        "For the bonus task you can implement the CAM method for the pneumonia classification of the very first MDL exercise sheet (Exercise 0).\n",
        "Therefore\n",
        "+ adapt the CNN accordingly to become usable with the CAM method\n",
        "+ extract the CAMs for the **test** data with the index $i\\in\\{0, 14, 15, 26, 27, 33, 34\\}$ (including only samples with pneumonia)\n",
        "+ visualize them with matplotlib or a similar plotting library of your choice\n",
        "+ have a closer look at the activation maps. Do you notice something, that indicates an overfitting on the dataset or a false decision base of the CNN for the classification? **Write some short sentences!**\n",
        "\n",
        "Your results should look similar to these plots:\n",
        "\n",
        "![cam0](https://drive.google.com/uc?export=view&id=18JBnp637Z6WKHpiQmFUWlGHb-cBfnaEH)\n",
        "![cam14](https://drive.google.com/uc?export=view&id=1U1os6B4m-FAxCq4TB50jlnTZb5HD9Kn7)\n",
        "![cam15](https://drive.google.com/uc?export=view&id=1IyNItiLOvbFD-Uqs8yVMM98-4c6rtqtW)\n",
        "![cam26](https://drive.google.com/uc?export=view&id=1iDmRY3gRoeTwXN6Njrhv5mZqR_Jdvcco)\n",
        "![cam27](https://drive.google.com/uc?export=view&id=1RxxEgtK47IBVWBoGQR9QusUWYXhiVlxD)\n",
        "![cam33](https://drive.google.com/uc?export=view&id=1hhmSVIKkecQvmFnjKPo_CnhGMRtwjoaJ)\n",
        "![cam34](https://drive.google.com/uc?export=view&id=14sWeMsTnSx5V2tLcynXvT8qAb2MVhMHA)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "LSZWyN7jUXnT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}