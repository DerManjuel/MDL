{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Deep Learning\n",
    "## Exercise 5: Model Distillation & Ternary Nets\n",
    "\n",
    "The goal of this exercise is to implement methods that allow to compress deep learning models via model distillation and ternary weights. This enables the use of deep learning in medicine due to its real-time ability and implementation on weaker mobile devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRUWE-KitK2d",
    "outputId": "bbcc009f-c058-41ff-cb1f-0cb358a3c0a6"
   },
   "outputs": [],
   "source": [
    "#run pip install for pytorch flop counter before first use\n",
    "!pip install onnx wget\n",
    "!pip install --upgrade git+https://github.com/Lyken17/pytorch-OpCounter.git\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil,gzip\n",
    "import wget\n",
    "\n",
    "#some functions to count unique parameters and sparsity are provided\n",
    "def countParameters(net):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "    params = sum([p.numel() for p in model_parameters])\n",
    "    return params\n",
    "\n",
    "def countUnique(net):\n",
    "    unique = 0\n",
    "    for m in net.modules():\n",
    "        if(isinstance(m,nn.Conv2d)):\n",
    "            unique += len(np.unique(m.weight.data.cpu().flatten().numpy()))\n",
    "    return unique\n",
    "    #print('#unique',unique)\n",
    "\n",
    "def countSparsity(net):\n",
    "    count_nonzero = 0; count_zero = 0\n",
    "    for m in net.modules():\n",
    "        if(isinstance(m, nn.Conv2d)):\n",
    "            count_nonzero += torch.sum((m.weight.data!=0).float())\n",
    "            count_zero += torch.sum((m.weight.data==0).float())\n",
    "    return count_zero/(count_zero+count_nonzero)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We will use the data of the Patch Camelyon (tupac16) Challenge. It consists of $327\\,680$ color images extracted from histopathologic scans of lymph node sections. The task is to classify the presence of metastatic tissue (global binary labels are given). The images were preprocessed to a spatial dimension of $48\\times48$ and split to 65k for training and 16k for testing images. See [here](https://www.kaggle.com/competitions/histopathologic-cancer-detection/overview) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "0kHfFFKUtcTY",
    "outputId": "d4eb6233-6965-400e-e421-37a7e4583bc5"
   },
   "outputs": [],
   "source": [
    "#loading the patch-based wholeslide histopathology data (uint8) and converting it to torch tensors\n",
    "import os\n",
    "\n",
    "dataset_url = 'https://cloud.imi.uni-luebeck.de/s/sjjiReHqSnokJ2n/download'\n",
    "\n",
    "def get_data(data_url):\n",
    "    filename = './patchCamelyon8c.mat'\n",
    "    if not os.path.exists(filename):\n",
    "        filename = wget.download(data_url)\n",
    "    \n",
    "get_data(dataset_url)\n",
    "\n",
    "\n",
    "data = scipy.io.loadmat('patchCamelyon8c.mat')\n",
    "\n",
    "img_train = torch.from_numpy(data['img_train'].astype('float32')/255)\n",
    "img_test = torch.from_numpy(data['img_test'].astype('float32')/255)\n",
    "\n",
    "label_train = torch.from_numpy(data['label_train']).long()\n",
    "label_test = torch.from_numpy(data['label_test']).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize an example for the two classes. You can run the cell multiple times, getting each time new random examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "0kHfFFKUtcTY",
    "outputId": "d4eb6233-6965-400e-e421-37a7e4583bc5"
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2)\n",
    "idx_negative = torch.where(label_train.squeeze() == 0)[0]\n",
    "idx_negative = idx_negative[torch.randperm(len(idx_negative))[0]]\n",
    "\n",
    "idx_positive = torch.where(label_train.squeeze() == 1)[0]\n",
    "idx_positive = idx_positive[torch.randperm(len(idx_positive))[0]]\n",
    "\n",
    "print('index for no metastatic tissue:', idx_negative.item())\n",
    "print('index for metastatic tissue:', idx_positive.item())\n",
    "\n",
    "ax1.imshow(img_train[idx_negative].permute(1, 2, 0))\n",
    "ax1.set_title('no metastatic tissue')\n",
    "ax2.imshow(img_train[idx_positive].permute(1, 2, 0))\n",
    "ax2.set_title('metastatic tissue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a pretrained `VGG11`, inspect its architecture and gather some computational information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKHf1__JtM6x",
    "outputId": "d3e25ea3-1a46-4212-a159-4a539c75682b"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "net = torchvision.models.vgg11_bn(pretrained='True')\n",
    "\n",
    "summary(net, (128, 3, 48, 48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0 (15 points): Modify a pretrained VGG11_BN network for the given training data\n",
    "Complete the function below, which returns a VGG11-Net with its architecture modified accordingly to match the tupac16 dataset.\n",
    "+ Replace the layer `net.avgpool` with an adaptive average pool of output size $1\\times1$\n",
    "+ Create a new classifier as `nn.Sequential` with two linear layers ($512\\times256$ and $256\\times2$) including one ReLU and no batch-norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tupac16_vgg11():\n",
    "    net = torchvision.models.vgg11_bn(pretrained='True')\n",
    "    net.avgpool = # todo\n",
    "    net.classifier = # todo\n",
    "    \n",
    "    return net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the reduced parameter count using `summary` from `torchinfo`. Your should obtain about $9\\,357\\,826$ parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task1 (25 points): Fine tuning\n",
    "Fine-tune this network for 16 sub-epochs on the tupac16 dataset. One sub-epoch is defined as a random quarter of the training pathes. Use `torch.randperm` to generate the needed indices for every epoch. The batch size should be 128. Choose Adam as an optimizer with an initial learning rate of 0.0005 and an exponential learning rate scheduler with `gamma=0.9`. After training, evaluate the model on the test data. It should yield an accuracy about $94\\%$.\n",
    "\n",
    "**Note:** Task 2 and 3 can be performed independently, but you should store each trained network under a new filename (for comparisons). In the following all techniques should only be applied to the feature-part of the network (and not the classifier layers).\n",
    "\n",
    "**Hint**: If you struggle with implementing of the training routine, have a look at the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqKw4fzMVd4Q",
    "outputId": "ea540373-d6c8-44a4-8c9b-8b6171915692"
   },
   "outputs": [],
   "source": [
    "# Task1 - Training loop\n",
    "\n",
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (60 points): Network Pruning through increased Sparsity\n",
    "+ Start with the same modified, pre-trained (not fine-tuned) vgg11_bn as before by calling your `tupac16_vgg11` method. \n",
    "+ Reuse the training routine from above.\n",
    "+ Add a sparsity promoting L1-loss (sum of absolute values) with a weight factor of 0.04 to the classification loss on the weights and bias of each BatchNorm2d.\n",
    "    + Therefor iterate over all modules of the net using `modules()` and determine the layer type using `isinstance()`\n",
    "+ Retrain the network.\n",
    "+ Evaluate its test accuracy (will drop slightly to ~89%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 872,
     "referenced_widgets": [
      "a4b2c1e9e9da45218d674680eee4a21f",
      "5dc5d66a6ea84d44ad8bb775d5064107",
      "58c56bc083c54fad82074619c859fc83",
      "1bcd770ca750483f9b79fc8d74033c01",
      "c1fcdfb88c974678852d275cf01f4b76",
      "770775fa63034b31af1ee2d27e241a2c",
      "bdda5b054ee0490c87229bb77e01810e",
      "3d942dfce94b4984bc54e13413fe6ee0"
     ]
    },
    "id": "nLFzc3vmm-qi",
    "outputId": "87b88b2c-806d-4d1c-f2e3-eeca3c9254bd"
   },
   "outputs": [],
   "source": [
    "# Task 2\n",
    "\n",
    "# todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that determines a threshold for input/output neurons to be set to zero (the ones which have been reduced in absolute value using the sparsity constraint). You can use the function `topk`, which outputs both the values and indices sorted around a chosen quantile/percentile. Here we simply use the median to set 50% of values to zero.\n",
    "\n",
    "When applied correctly (as incoming & outgoing mask) for each Conv2d layer, it reduces the nonzero parameters by ~75% (the first incoming & last outgoing Conv2d are not masked). \n",
    "Note that BatchNorm has four tensors and two index masks have to be applied as follows:\n",
    "\n",
    "`B = A[idx_next,:,:,:][:,idx_prev,:,:]`\n",
    " \n",
    "Now you can replace all Conv2d and BatchNorm2d layers with smaller filters (and copy their weights) so that we have the following sequence of channels: 3, 32, 64, (2x)128, (3x)256, 512.\n",
    "Evaluate the slimmed network (you could observe a slight improvement to ~92%) and confirm that the required computations are reduced to 12 GFlops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0DMYXwivPPe",
    "outputId": "ee784a45-8b21-49a9-c069-6d97ecef2311"
   },
   "outputs": [],
   "source": [
    "# task 2 Network slimming (construct lean filters)\n",
    "\n",
    "sparsity_s = 0.04\n",
    "ternary = False\n",
    "\n",
    "model_name = 'tupac_sparse'\n",
    "\n",
    "net = torch.load(model_name+'_net.pth')\n",
    "\n",
    "net.eval()\n",
    "net.cuda()\n",
    "\n",
    "mask = torch.ones(3).cuda()\n",
    "idx_prev = torch.arange(3).long().cuda()\n",
    "q75 = 0.5 \n",
    "print('#params before',countParameters(net.features))\n",
    "print('#features sparsity',countSparsity(net.features))\n",
    "\n",
    "\n",
    "for c in range(len(net.features)-2):\n",
    "    if isinstance(net.features[c], nn.Conv2d):\n",
    "        if(c==25):\n",
    "            q75=1\n",
    "        # todo\n",
    "\n",
    "\n",
    "# evaluation for task 2\n",
    "        \n",
    "print('#params after',countParameters(net.features))\n",
    "print('#features sparsity',countSparsity(net.features))    \n",
    "\n",
    "idx_epoch = torch.arange(16384).view(128,-1)\n",
    "val_acc = 0\n",
    "\n",
    "for iter in range(idx_epoch.size(1)):\n",
    "    idx_iter = idx_epoch[:,iter]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #forward path and loss\n",
    "        outputs = net(img_test[idx_iter,:,:,:].cuda())\n",
    "    val_acc += torch.sum((outputs.argmax(1).cpu()==label_test[0,idx_iter]).float())/16384.0\n",
    "\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Task - Ternary Nets:\n",
    "Start with the same modified, pre-trained (not fine-tuned) vgg11_bn as before. Finally, we want to explore, how the memory storage can be reduced with little loss. Here a ternary weight approximation will be used for which we first estimate a per-channel $\\Delta$ for each weight in Conv2d given the rule-of-thumb below.\n",
    "\n",
    "$\\begin{align}\n",
    "    \\Delta &= \\frac{0.7}{n}\\sum^n_{i=1}|W_i|\\\\\n",
    "    \\tilde{W}_i&=\n",
    "    \\begin{cases}\n",
    "        +1, &\\text{ if } W_i > \\Delta\\\\\n",
    "        0,  &\\text{ if } |W_i| \\leq \\Delta\\\\\n",
    "        -1,  &\\text{ else }\n",
    "    \\end{cases}\\\\\n",
    "    n_\\Delta &= \\sum_i|\\tilde{W}_i|\\\\\n",
    "    \\alpha &= \\frac{1}{n_\\Delta}\\sum_i|\\tilde{W}_i||W_i|\n",
    "\\end{align}$\n",
    "\n",
    "Tip: after calculating the absolute values the mean has to be computed over all but the 0-th dimension.\n",
    "The obtained ternary weights have lost their magnitude, therefore the parameter $\\alpha$ (again per-channel) is computed and multiplied with the weight tensor.\n",
    "\n",
    "Test your function with the check implemented below. For a $128\\times64\\times3\\times3$ kernel the number of unique entries is reduced from more than 70 thousand to just 257 ($2 \\cdot 128 + 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4ay90rCv3gi"
   },
   "outputs": [],
   "source": [
    "# template for function in bonus task\n",
    "def approx_weights(w_in,flag=True):\n",
    "    if(flag):\n",
    "        with torch.no_grad():\n",
    "            a,b,c,d = w_in.size()\n",
    "            delta = # todo\n",
    "            alpha = # todo\n",
    "            w_out = # todo\n",
    "    else:\n",
    "        w_out = w_in\n",
    "    return w_out\n",
    "\n",
    "# check the number of unique values before/after ternary approximation \n",
    "w_in = net.features[8].weight.clone().detach()\n",
    "w_approx = approx_weights(w_in,True)\n",
    "print('#unique',len(np.unique(w_in.data.cpu().flatten().numpy())))\n",
    "print('#unique',len(np.unique(w_approx.data.cpu().flatten().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4ay90rCv3gi"
   },
   "source": [
    "To effectively train a network with weight quantisation, it is important to only use the ternary weights during forward/backward path, but update their gradients in full precision.\n",
    "\n",
    "Implement a loop that stores full precision weights in a list of tensors and replaces the `.data` values with their approximation just before calling the forward pass (and zero_grad).\n",
    "Reassign these backup copies after `loss.backward()` and `before optimizer.step()`. Retrain your network and take care to perform the weight quantisation the same way during test evaluation. The test accuracy should be around 85-90% during the epochs.\n",
    "\n",
    "**Tip:** you could use `.pop(0)` to (iteratively) access and remove the first object of a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mdl_exercise5-Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1bcd770ca750483f9b79fc8d74033c01": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d942dfce94b4984bc54e13413fe6ee0",
      "placeholder": "​",
      "style": "IPY_MODEL_bdda5b054ee0490c87229bb77e01810e",
      "value": " 507M/507M [05:43&lt;00:00, 1.55MB/s]"
     }
    },
    "3d942dfce94b4984bc54e13413fe6ee0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c56bc083c54fad82074619c859fc83": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_770775fa63034b31af1ee2d27e241a2c",
      "max": 531503671,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c1fcdfb88c974678852d275cf01f4b76",
      "value": 531503671
     }
    },
    "5dc5d66a6ea84d44ad8bb775d5064107": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "770775fa63034b31af1ee2d27e241a2c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4b2c1e9e9da45218d674680eee4a21f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58c56bc083c54fad82074619c859fc83",
       "IPY_MODEL_1bcd770ca750483f9b79fc8d74033c01"
      ],
      "layout": "IPY_MODEL_5dc5d66a6ea84d44ad8bb775d5064107"
     }
    },
    "bdda5b054ee0490c87229bb77e01810e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1fcdfb88c974678852d275cf01f4b76": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
