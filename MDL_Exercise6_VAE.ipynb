{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerManjuel/MDL/blob/main/MDL_Exercise6_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "I847WdiOYc1H"
      },
      "source": [
        "# <u>M</u>edical <u>D</u>eep <u>L</u>earning\n",
        "## Exercise 6: Variational Auto-Encoder\n",
        "\n",
        "The goal of this exercise is to implement an unsupervised method to adapt the data distribution of our well known Pneumonia Torso X-Ray dataset of the very first exercise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_6nxDirmYc1U"
      },
      "source": [
        "![VAE](https://cloud.imi.uni-luebeck.de/s/fBDPZdQGajmcfeN/download)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bX05QJIuYc1V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nmGfih8NYc1X"
      },
      "source": [
        "## Dataset implementation\n",
        "Provided dataset implementation of 0th exercise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "l_-VQ6XVYc1Y"
      },
      "outputs": [],
      "source": [
        "import wget\n",
        "from os.path import exists\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class PneumoniaXRayDataset(Dataset):\n",
        "    def __init__(self, mode: str):\n",
        "        super(PneumoniaXRayDataset, self).__init__()\n",
        "        # load data\n",
        "        data_file = 'pneumonia_detection_data_img.pth'\n",
        "        label_file = 'pneumonia_detection_data_label.pth'\n",
        "\n",
        "        if not exists(data_file):\n",
        "            data_file = wget.download(\n",
        "                'https://cloud.imi.uni-luebeck.de/s/7Re8XdKCxpTZDk2/download/pneumonia_detection_data_img.pth')\n",
        "        if not exists(label_file):\n",
        "            label_file = wget.download(\n",
        "                'https://cloud.imi.uni-luebeck.de/s/XgccQesxFeT9EqE/download/pneumonia_detection_data_label.pth')\n",
        "\n",
        "        img = torch.load(data_file)\n",
        "        lbl = torch.load(label_file)\n",
        "\n",
        "        img = (img - img.min()) / (img.max() - img.min())\n",
        "        img = img * 2 - 1\n",
        "\n",
        "        torch.manual_seed(42)\n",
        "        rand_idx = torch.randperm(len(lbl))\n",
        "\n",
        "        # split data\n",
        "        if mode == 'train':\n",
        "            self.idx = rand_idx[:4500]\n",
        "        elif mode == 'val':\n",
        "            self.idx = rand_idx[4500:5000]\n",
        "        elif mode == 'test':\n",
        "            self.idx = rand_idx[5000:]\n",
        "        else:\n",
        "            raise ValueError(f'unknown mode: {mode}. Valid modes are train, val and test')\n",
        "\n",
        "        self.img = img[self.idx]\n",
        "        self.lbl = lbl[self.idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lbl)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.img[idx], self.lbl[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uKqSBUAdYc1Z"
      },
      "source": [
        "## Visualisation of some random samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pmYQD99XYc1a"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(3, 2, figsize=(10, 10))\n",
        "ds = PneumoniaXRayDataset('train')\n",
        "\n",
        "for i, ax in enumerate(axs):\n",
        "    ax[0].imshow(ds.img[ds.lbl == 0][i].squeeze(), cmap='gray')\n",
        "    ax[0].axis('off')\n",
        "    ax[0].set_title('healthy')\n",
        "\n",
        "    ax[1].imshow(ds.img[ds.lbl == 1][i].squeeze(), cmap='gray')\n",
        "    ax[1].axis('off')\n",
        "    ax[1].set_title('pneumonia')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PNrNQfHEYc1b"
      },
      "source": [
        "# **Task 1: Build an encoder-decoder architecture and train with KLD loss**\n",
        "\n",
        "Create an encoder that takes $B\\times1\\times128\\times128$ input images and produces two 512 dim. latent vectors ($\\mu$ and $\\log(\\sigma^2)$). Therefore implement the `CBR` function that returns two Conv2d → BNorm → LeakyReLU groups wrapped in a `nn.Sequential`. Both Convs have `kernel_size=3` and `padding=1`. The second one also reduces the spatial resolution by factor 2 via `stride=2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RP3CoN8TYc1c"
      },
      "outputs": [],
      "source": [
        "def CBR(in_channel, out_channel):\n",
        "    # todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KKX3LDtdYc1d"
      },
      "source": [
        "The decoder takes a $B\\times512\\times1\\times1$ $z$-vector and generates a fully-sized ($B\\times1\\times128\\times128$) output image. So its convolutional building blocks returned by `upCBR` have to reconstruct the spatial dimension. This is done via `nn.Upsample` with `mode=bilinear` and `align_corners=False` followed by a Conv2d → BNorm → LeakyReLU group with `kernel_size=3` and `padding=1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YpoI36R6Yc1f"
      },
      "outputs": [],
      "source": [
        "def upCBR(in_channel, out_channel):\n",
        "    # todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "sgWmTTjZYc1k"
      },
      "source": [
        "Now, let's build the VAE. The encoder takes $B\\times1\\times128\\times128$ input images and produces a 128 dim. latent vector, that are used to generate the 512 dim $\\mu$ and $\\log(\\sigma^2)$ vectors via two fully connected $1\\times1$ convolutions `fc_mu`and `fc_log_var`.\n",
        "\n",
        "The encoder consists of four `CBR` blocks:\n",
        "+ `CBR(1, 16)`\n",
        "+ `CBR(16, 32)`\n",
        "+ `CBR(32, 64)`\n",
        "+ `CBR(64, 64)`\n",
        "\n",
        "followed by a last Conv2d → BNorm → LeakyReLU group, that project the $B\\times64\\times8\\times8$ to a $B\\times128\\times1\\times1$ vector. Choose its kernel size appropriate.\n",
        "\n",
        "Next, implement the decoder, which takes a $B\\times512\\times1\\times1$ $z$-vector and generates a fully-sized ($B\\times1\\times128\\times128$) output image. Start with a `ConvTranspose2d`  with kernel=8, channel-out=64 and no padding, followed by four `upCBR`:\n",
        "\n",
        "+ `upCBR(64, 32)`\n",
        "+ `upCBR(32, 16)`\n",
        "+ `upCBR(16, 8)`\n",
        "+ `upCBR(8, 8)`\n",
        "\n",
        "Close the VAE with a $1\\times1$ convolution to project the channel to one single output channel, followed by a `nn.Tanh`.\n",
        "\n",
        "Build a VAE with your encoder-decoder architecture, you can follow [this example](https://github.com/pytorch/examples/blob/master/vae/main.py).\n",
        "**Note**: that `mu` and `log_var` have to be returned after the forward path for loss calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sXlDQVjCYc1m"
      },
      "outputs": [],
      "source": [
        "# Skeleton for task 1\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VAE, self).__init__()\n",
        "        self.fc_mu = # todo\n",
        "        self.fc_log_var = # todo\n",
        "\n",
        "        self.encoder = # todo\n",
        "\n",
        "\n",
        "        self.decoder = # todo\n",
        "\n",
        "    def encode(self, x):\n",
        "        # get encoding\n",
        "        # todo\n",
        "\n",
        "        # predict distribution parameters\n",
        "        log_var = # todo\n",
        "        mu = # todo\n",
        "\n",
        "        # reparameterize\n",
        "        z = # todo\n",
        "\n",
        "        return z, mu, log_var\n",
        "\n",
        "    def forward(self, x):\n",
        "        # todo\n",
        "\n",
        "        return x_hat, mu, log_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "tygjDr8iYc1n"
      },
      "source": [
        "Check your VAE's architecture using the `summary` function from `torchinfo` with a batch size of 1. You should obtain about $2\\,925\\,433$ parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DfjXoJuOYc1o"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "n6KAb3ZNYc1o"
      },
      "source": [
        "## L1 and KLD Loss\n",
        "\n",
        "Implement the loss function, consisting of the L1 distance between the image and the reconstructed one (use `F.F.l1_loss`) and the kullback leibler divergence following this formula:\n",
        "$$\\text{KLD}= -0.5\\cdot\\frac{1}{N}\\cdot\\sum_n^N\\left(1+\\log(\\sigma_n^2)-\\mu^2_n-\\sigma_n^2\\right)$$\n",
        "Add both loss, but divide KLD by the number of pixels to make the reconstruction loss more important and return the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sDbd2lxeYc1o"
      },
      "outputs": [],
      "source": [
        "def loss_function(x_hat, x, mu, logvar):\n",
        "    \"\"\"\n",
        "    loss for unsupervised training of a VAE with L1-norm and KLD\n",
        "    :param x_hat: reconstructed image\n",
        "    :param x: original image\n",
        "    :param mu: mean of the predicted latent space distribution\n",
        "    :param logvar: logarithmic variation of the predicted latent space distribution\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # todo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OvmasG2nYc1p"
      },
      "source": [
        "## Training\n",
        "Train this network for 300 epochs with `batch_size=32` and `shuffle=True` using a `DataLoader` and a `Adam` optimizer with its default settings. Use the L1loss for the reconstruction of images and the Kullback-Leibler divergence (KLD) as loss function. Use the learning rate scheduler `MultiStepLR` to reduce the learning rate by a factor of 10 after 200 and 280 epochs (do not forget to call `step()` on the scheduler after each epoch). You should obtain a loss of about $0.018$ after 300 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "59u4rxUoYc1p"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange, tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchmetrics import MeanMetric\n",
        "\n",
        "data_laoder = DataLoader(dataset=PneumoniaXRayDataset(mode='train'),\n",
        "                         batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "model = # todo\n",
        "\n",
        "optimizer = # todo\n",
        "scheduler = # todo\n",
        "\n",
        "loss_history = []\n",
        "collector = MeanMetric(nan_strategy='error').cuda()\n",
        "for epoch in trange(300, unit='epoch'):\n",
        "    # train the VAE\n",
        "    model.train()\n",
        "\n",
        "    # for each mini-batch\n",
        "    for x, _ in data_laoder:\n",
        "        # forward\n",
        "        loss = # todo\n",
        "\n",
        "        # backward and optimize\n",
        "        # todo\n",
        "\n",
        "        # calculate mean loss\n",
        "        collector(loss)\n",
        "\n",
        "    # update learning rate\n",
        "    # todo\n",
        "\n",
        "    # track loss\n",
        "    loss_history.append(collector.compute().item())\n",
        "\n",
        "    # visualize some reconstructions\n",
        "    if epoch % 10 == 0:\n",
        "        tqdm.write(f'{epoch}: {collector.compute().item()} L1 + KDE loss')\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            x_hat, _, _ = model(x)\n",
        "            fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
        "\n",
        "            ax[0, 0].imshow(x[0].squeeze().cpu(), cmap='gray')\n",
        "            ax[0, 1].imshow(x_hat[0].squeeze().cpu(), cmap='gray')\n",
        "\n",
        "            ax[1, 0].imshow(x[1].squeeze().cpu(), cmap='gray')\n",
        "            ax[1, 1].imshow(x_hat[1].squeeze().cpu(), cmap='gray')\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    collector.reset()\n",
        "\n",
        "torch.save(model, 'vae_task1.pt')\n",
        "del model\n",
        "\n",
        "# plot loss history\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('overview')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.title('zoomed in')\n",
        "plt.ylim(min(loss_history) * 0.9, 0.03)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "pv_T2Q4kYc1q"
      },
      "source": [
        "# Task 2: Smooth interpolation between samples\n",
        "One big advantage of a VAE is its continuous and so interpretable latent space. We can demonstrate this by creating two $z$ vectors of two image and interpolating them in the $z$-space obtaining smooth transition images.\n",
        "\n",
        "First we encode the two given images `img1` and `img2` to their $z$ vectors `z_1` and `z_2` using the encoder of our trained VAE. Next, we create the `z` tensor, that holds $n\\times512\\times1\\times1$ interpolated $z$ vectors. To obtain the weight for the interpolation use `torch.linspace(0, 1, n)`. Reconstruct the images with the decoder of the VAE. Use the provided function `plot_transition_img` to plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r2d_0zBUYc1q"
      },
      "outputs": [],
      "source": [
        "# provided function for plotting the transition using a tensor of shape [nx1x128x128]\n",
        "def plot_transition_img(imgs: torch.Tensor, n: int):\n",
        "    fig, ax = plt.subplots(1, n, figsize=(20,3))\n",
        "    fig.suptitle('Reconstructions')\n",
        "    for i in range(n):\n",
        "        ax[i].imshow(reconstructed_imgs[i].squeeze(), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = torch.load('vae_task1.pt').cpu().eval()\n",
        "ds = PneumoniaXRayDataset('train')\n",
        "\n",
        "n = 8\n",
        "\n",
        "img1 = ds.img[1]\n",
        "img2 = ds.img[5]\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20,5))\n",
        "fig.suptitle('Original images')\n",
        "ax[0].imshow(img1.squeeze(), cmap='gray')\n",
        "ax[1].imshow(img2.squeeze(), cmap='gray')\n",
        "\n",
        "with torch.no_grad():\n",
        "    z_1 = # todo\n",
        "    z_2 = # todo\n",
        "\n",
        "z = # todo\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstructed_imgs = # todo\n",
        "\n",
        "plot_transition_img(reconstructed_imgs, n)\n",
        "del model"
      ],
      "metadata": {
        "id": "4pLPAjBOYc1q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wIZG2B1KYc1r"
      },
      "source": [
        "# **Bonus task: Implement a perceptual loss and add this to your VAE training  (5 points)**\n",
        "\n",
        " A pre-trained fully-convolutional network which encoder is adapted to the VAE's one has been trained for the pneumonia classification and is provided for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "!wget -nc https://drive.google.com/file/d/1BairhYZs3uGvjbcgHC_IUD5NCsAdefT4/view?usp=sharing -O pneumonina_cnn.py\n",
        "!wget -nc https://drive.google.com/file/d/1W_9kfN_Vi7DdkDBDs5sIwCHW8arzRyeK/view?usp=sharing -O pneumonia_model.pt"
      ],
      "metadata": {
        "id": "7YWNDD5RYc1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dJlez_-wYc1s"
      },
      "outputs": [],
      "source": [
        "from pneumonia_cnn import *\n",
        "\n",
        "pretrainend_classification_model = torch.load('pneumonia_model.pt').eval().cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "PHvhIHcwYc1s"
      },
      "source": [
        "## Perceptual Loss function\n",
        "Complete the `perceptual_loss` function. Index the encoder of the pretrained model and of the VAE and calculate the difference between their feature using the `F.l1_loss`. Feature should be extracted after the last activation function of the spatial resolution $64^2, 32^2, 16^2$. Sum up the losses and return the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "USigm2x7Yc1s"
      },
      "outputs": [],
      "source": [
        "def perceptual_loss(pretrainend_model, model, x):\n",
        "    # feature pretrainend model\n",
        "    # todo\n",
        "\n",
        "    # feature model\n",
        "    # todo\n",
        "\n",
        "    # calculate loss\n",
        "    loss = # todo\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "VGuOx9bAYc1s"
      },
      "source": [
        "Retrain the VAE with the same training routine like task 1, but with the additional perceptual loss. The visual results will get a little more blurry, but the training converge faster and is able to already produce some meaningfully results in the very first epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#implement the perceptual loss and retrain your VAE\n",
        "\n",
        "# todo\n",
        "\n",
        "torch.save(model, 'vae_perceptual_loss.pt')"
      ],
      "metadata": {
        "id": "fZjEz8sGYc1t"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}