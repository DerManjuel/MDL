{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerManjuel/MDL/blob/main/MDL_Exercise3_registration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkcnBlOokZpS"
      },
      "source": [
        "# Medical Deep Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4ZR-gga246T"
      },
      "source": [
        "## Exercise 3: Learning-based multi-modal 3D registration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap6DjNxPTY1A"
      },
      "source": [
        "The aim of this exercise is to introduce you to deep-learning based image registration and also explore mutual information as a metric to supervise the training of multi-modal feature networks. The method we want to implement comprises three parts: \n",
        "\n",
        "1.   A global mutual information loss function\n",
        "2.   A correlation layer to robustly estimate large rigid transformations\n",
        "3.   A compact 3D CNN network (with some modality specific and some shared layers) to predict features suitable for multi-modal CT/MR registration\n",
        "\n",
        "**Provided functions and data loading**\n",
        "\n",
        "![thorax](https://drive.google.com/uc?export=view&id=1b8Y6YRMl-YTe6tdSysY2C1dKA9Vvqh6y)\n",
        "\n",
        "We have prepared a dataset with CT and MRI scan pairs of same patients from TCIA (the Cancer Imaging Archive) and also provide you with manual annotations to evaluate the method (those are not necessary for training).\n",
        "The image dimensions are $192\\times160\\times192$, there are 8 (pre-aligned) scan pairs that will be augmented to 64 pairs using random rigid transformations and each scan has 4 anatomical labels: liver, spleen, right kidney and left kidney (some patients have only one kidney).\n",
        "\n",
        "The following cells provide the fundamental for loading and augmenting the data. Also, you are given a number of functions for the subsequent registration tasks. You are invited to read and understand the code if you are interested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Pd-pMNQcTY1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f2c746-98b9-4803-f983-43c576155024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "--2023-05-31 11:31:55--  https://cloud.imi.uni-luebeck.de/s/76KJ7RBqpsdjSbw/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1638735 (1.6M) [application/octet-stream]\n",
            "Saving to: ‘mdl3_masks.npz’\n",
            "\n",
            "mdl3_masks.npz      100%[===================>]   1.56M  2.43MB/s    in 0.6s    \n",
            "\n",
            "2023-05-31 11:31:56 (2.43 MB/s) - ‘mdl3_masks.npz’ saved [1638735/1638735]\n",
            "\n",
            "--2023-05-31 11:31:56--  https://cloud.imi.uni-luebeck.de/s/yAZNkTBRGoeePZa/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 188744833 (180M) [application/octet-stream]\n",
            "Saving to: ‘mdl3_imgs.pth’\n",
            "\n",
            "mdl3_imgs.pth       100%[===================>] 180.00M  27.0MB/s    in 7.4s    \n",
            "\n",
            "2023-05-31 11:32:04 (24.3 MB/s) - ‘mdl3_imgs.pth’ saved [188744833/188744833]\n",
            "\n",
            "--2023-05-31 11:32:04--  https://cloud.imi.uni-luebeck.de/s/nSixsneJ6fDbfBB/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64883639 (62M) [application/octet-stream]\n",
            "Saving to: ‘mdl3_exercise_task12.pth’\n",
            "\n",
            "mdl3_exercise_task1 100%[===================>]  61.88M  21.1MB/s    in 2.9s    \n",
            "\n",
            "2023-05-31 11:32:07 (21.1 MB/s) - ‘mdl3_exercise_task12.pth’ saved [64883639/64883639]\n",
            "\n",
            "--2023-05-31 11:32:07--  https://cloud.imi.uni-luebeck.de/s/X8A8Dixgj62Qwtf/download\n",
            "Resolving cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)... 141.83.20.118\n",
            "Connecting to cloud.imi.uni-luebeck.de (cloud.imi.uni-luebeck.de)|141.83.20.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3249 (3.2K) [text/x-python]\n",
            "Saving to: ‘mdl_exercise3_utils.py’\n",
            "\n",
            "mdl_exercise3_utils 100%[===================>]   3.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-05-31 11:32:08 (50.7 MB/s) - ‘mdl_exercise3_utils.py’ saved [3249/3249]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo\n",
        "import torchinfo\n",
        "# Download train and test data\n",
        "!wget -nc https://cloud.imi.uni-luebeck.de/s/76KJ7RBqpsdjSbw/download -O mdl3_masks.npz\n",
        "!wget -nc https://cloud.imi.uni-luebeck.de/s/yAZNkTBRGoeePZa/download -O mdl3_imgs.pth\n",
        "\n",
        "# Download feature data for testing task 1 & 2\n",
        "!wget -nc https://cloud.imi.uni-luebeck.de/s/nSixsneJ6fDbfBB/download -O mdl3_exercise_task12.pth\n",
        "\n",
        "# Download and import an additional python file providing utility functions\n",
        "!wget -nc https://cloud.imi.uni-luebeck.de/s/X8A8Dixgj62Qwtf/download -O mdl_exercise3_utils.py\n",
        "\n",
        "from mdl_exercise3_utils import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gGoD1bGfTY1B"
      },
      "outputs": [],
      "source": [
        "# some parameters\n",
        "\n",
        "grid_step = 12\n",
        "disp_radius = 4\n",
        "disp_step = 5\n",
        "beta = 25\n",
        "\n",
        "W = D = 192\n",
        "H = 160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C18Adl-STY1C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a10ab3-b28d-44e5-a051-798bb458e08b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ],
      "source": [
        "def least_trimmed_rigid(fixed_pts, moving_pts, iter=5):\n",
        "    idx = torch.arange(fixed_pts.shape[0]).to(fixed_pts.device)\n",
        "    for i in range(iter):\n",
        "        x = find_rigid_3d(fixed_pts[idx,:], moving_pts[idx,:]).t()\n",
        "        residual = torch.sqrt(torch.sum(torch.pow(moving_pts - torch.mm(fixed_pts, x), 2), 1))\n",
        "        _, idx = torch.topk(residual, fixed_pts.shape[0]//2, largest=False)\n",
        "    return x.t().to(fixed_pts.dtype)\n",
        "\n",
        "def find_rigid_3d(x, y):\n",
        "    x_mean = x[:, :3].mean(0)\n",
        "    y_mean = y[:, :3].mean(0)\n",
        "    u, s, v = torch.svd(torch.matmul((x[:, :3]-x_mean).t(), (y[:, :3]-y_mean)).float())\n",
        "    m = torch.eye(v.shape[0], v.shape[0]).to(x.device)\n",
        "    m[-1,-1] = torch.det(torch.matmul(v, u.t()).float())\n",
        "    rotation = torch.matmul(torch.matmul(v, m), u.t())\n",
        "    translation = y_mean - torch.matmul(rotation, x_mean)\n",
        "    T = torch.eye(4).to(x.device)\n",
        "    T[:3,:3] = rotation\n",
        "    T[:3, 3] = translation\n",
        "    return T\n",
        "\n",
        "def generate_random_rigid_3d(strength=.3):\n",
        "    x = torch.randn(12,3).to(device)\n",
        "    y = x + strength*torch.randn(12,3).to(device)\n",
        "    return find_rigid_3d(x, y)\n",
        "\n",
        "disp = torch.stack(torch.meshgrid(\n",
        "    torch.arange(- disp_step * disp_radius, disp_step * disp_radius + 1, disp_step),\n",
        "    torch.arange(- disp_step * disp_radius , disp_step * disp_radius  + 1, disp_step),\n",
        "    torch.arange(- disp_step * disp_radius , disp_step * disp_radius  + 1, disp_step))).permute(1, 2, 3, 0).contiguous().view(1, 1, -1, 1, 3).float()\n",
        "\n",
        "disp = (disp.flip(-1) * 2 / (torch.tensor([W, H, D]) - 1))#.to(dtype).to(device)\n",
        "\n",
        "def get_displacement():\n",
        "    return disp\n",
        "    \n",
        "disp_width = disp_radius * 2 + 1\n",
        "\n",
        "#finding 50% best matches and computing soft-correspondences is provided as \"robust_rigid_fit\"\n",
        "#in initial notebook, it receives the ssd_cost tensor N x 729 from Task 2 and returns a 4x4 matrix R\n",
        "\n",
        "def robust_rigid_fit(ssd_cost, kpts_fixed, feat_fix):\n",
        "    ssd_cost = ssd_cost.view(1,-1,(disp_radius*2+1)**3)\n",
        "    kpts_fixed = kpts_fixed.view(1,-1,3)\n",
        "    #mask_fix, feat_fix, feat_mov):#, grid_step, disp_radius, disp_step, beta=15):\n",
        "    #use predefined set of displacements\n",
        "    disp1 = disp.to(ssd_cost.device).to(ssd_cost.dtype)\n",
        "    \n",
        "    #remove 50% least reliable control points based on the minimum cost of their respective values\n",
        "    ssd_val, ssd_idx = torch.min(ssd_cost.squeeze(), 1)\n",
        "    idx_best = torch.sort(ssd_val, dim=0, descending=False)[1][:kpts_fixed.shape[1]//2]\n",
        "    #compute a weighted soft correspondence (displacement)\n",
        "    #this step is crucial to keep the loss differentiable!\n",
        "    disp_best = torch.sum(torch.softmax(-beta*ssd_cost.squeeze(0).unsqueeze(2),1) * disp1.view(1, -1, 3), 1)\n",
        "    disp_best = disp_best[idx_best,:]\n",
        "    \n",
        "    #compute absolute coordinates for coresspondences and run least trimmed squares fitting\n",
        "    fixed_pts = torch.cat((kpts_fixed[0,idx_best,:], torch.ones(idx_best.size(0),1).to(feat_fix.device).to(feat_fix.dtype)),1)\n",
        "    moving_pts = torch.cat((kpts_fixed[0,idx_best,:] + disp_best, torch.ones(idx_best.size(0),1).to(feat_fix.device).to(feat_fix.dtype)),1)\n",
        "    with torch.cuda.amp.autocast(enabled=False): #SVD is not available/stable with FP16\n",
        "        R = least_trimmed_rigid(fixed_pts.float(), moving_pts.float())\n",
        "    return R[:3,:4].unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VNLaCozYTY1C"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Run all required imports\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9KYSxyD-TY1D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140,
          "referenced_widgets": [
            "af50f4e22f104909961c5e984ec8ae44",
            "fa876b63714f47c09493aea69a6d3142",
            "34a0267b37564177b588fd44896d6536",
            "a1912fd90ec24643b6ea4fbf923730b0",
            "72218cd3c0aa49db91b05fe5e4a2c59c",
            "e4675427763a4ddb9e03fe2bd73dada8",
            "f6c870acd5ee416f828e98cc9a188344",
            "29cd38b3117442bea3f49cfb169779f3",
            "eceb3c02dabc46aba25158f5d1e47f45",
            "371da326217f4eeabf89f46cabf26ca4",
            "2494c23ec905440cb23cd56d09329cdc"
          ]
        },
        "outputId": "fef16dfd-d527-4c2f-b3ef-425ce470d389"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "load cases:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af50f4e22f104909961c5e984ec8ae44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4298: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#load data and create eight augmented versions each (64 CT/MR pairs in total)\n",
        "\n",
        "mdl3_imgs = torch.load('mdl3_imgs.pth')\n",
        "mdl3_masks = np.load('mdl3_masks.npz')\n",
        "\n",
        "def load_case(case):\n",
        "    img_fix = mdl3_imgs['mdl3_img_fix'][case].float().cpu()\n",
        "    img_mov = mdl3_imgs['mdl3_img_mov'][case].float().cpu()\n",
        "    mask_fix = torch.from_numpy(mdl3_masks['mdl3_mask_fix'][case]).cpu()\n",
        "    mask_mov = torch.from_numpy(mdl3_masks['mdl3_mask_mov'][case]).cpu()\n",
        "    seg_fix = torch.from_numpy(mdl3_masks['mdl3_seg_fix'][case]).cpu().long()\n",
        "    seg_mov = torch.from_numpy(mdl3_masks['mdl3_seg_mov'][case]).cpu().long()\n",
        "    return img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov\n",
        "\n",
        "\n",
        "TRAIN_CASES = torch.arange(8) \n",
        "\n",
        "imgs_fix_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).float()\n",
        "imgs_mov_train = torch.zeros(len(TRAIN_CASES), 1, D, H, W).float()\n",
        "segs_fix_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).int()\n",
        "segs_mov_train = torch.zeros(len(TRAIN_CASES), 1, D, H, W).int()\n",
        "masks_fix_train = torch.zeros(len(TRAIN_CASES), 8, D, H, W).bool()\n",
        "masks_mov_train = torch.zeros(len(TRAIN_CASES), 1, D, H, W).bool()\n",
        "for i, case in enumerate(tqdm(TRAIN_CASES, desc='load cases')):\n",
        "    img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov = load_case(case)\n",
        "    device = img_fix.device\n",
        "    img_fix = img_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    img_mov = img_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    seg_fix = seg_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    seg_mov = seg_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    mask_fix = mask_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    mask_mov = mask_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "    imgs_mov_train[i:i+1] = img_mov\n",
        "    segs_mov_train[i:i+1] = seg_mov\n",
        "    masks_mov_train[i:i+1] = mask_mov\n",
        "    for j in range(8):\n",
        "        with torch.no_grad():\n",
        "            R = generate_random_rigid_3d()\n",
        "            grid = F.affine_grid(R[:3,:4].unsqueeze(0).cuda(), (1,1,D,H,W))\n",
        "            img_fix_ = F.grid_sample(img_fix.cuda(), grid, padding_mode='border')\n",
        "            seg_fix_ = F.grid_sample(F.one_hot(seg_fix[0, 0]).permute(3, 0, 1, 2).unsqueeze(0).float().cuda(), grid).argmax(1, keepdim=True).int()\n",
        "            mask_fix_ = F.grid_sample(mask_fix.float().cuda(), grid)>0.5\n",
        "\n",
        "            imgs_fix_train[i:i+1, j:j+1] = img_fix_.cpu()\n",
        "            segs_fix_train[i:i+1, j:j+1] = seg_fix_.cpu()\n",
        "            masks_fix_train[i:i+1, j:j+1] = mask_fix_.cpu()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5MrOmxyTY1D"
      },
      "source": [
        "### Task 1: Computation of (joint) histograms and mutual information (25 points)\n",
        "\n",
        "Let's start with implementing mutual information as a loss function, that we can use later for supervised training. All input tensors have the shape $[N\\times C\\times D\\times H\\times W]$.\n",
        "\n",
        "* The values $v_i$ should be sampled of the (fixed) mask to exclude background locations.\n",
        "\n",
        "* Define a range for the histogram bins $b_i$ with `torch.linspace` with 64 steps and the minimum and maximum value of the respective image.\n",
        "\n",
        "* Compute the histograms $h_i$ using a Parzen window weighting with $\\sigma=0.015$: $$h_i=\\exp\\left(-\\frac{(v_i-b_i)^2}{2 \\cdot \\sigma^2}\\right)$$ (Broadcasting becomes again very handy here).\n",
        "\n",
        "* After this step your $h_{\\text{fix}}$ and $h_{\\text{mov}}$ should have a shape of  $[64 \\times N]$, where $N$ is the number of pixels (could be obtained with the sum over `mask_fix`).\n",
        "\n",
        "* Calculate the marginal (individual) $\\rho_i$ by summing/averaging over the pixels and normalizing the resulting vector by its sum (add a small $\\epsilon=1e-10$ for numeric stability). For the joint histogram the pairwise sums are implicitly obtained using a matrix multiplication of $h_{\\text{fix}}$ and transposed $h_{\\text{mov}}$. Do not forget to calculate its marginal too.\n",
        "\n",
        "* Calculate the entropy $E_i$ as $$E_i=-\\sum \\rho_i \\cdot \\log_2(\\rho_i + \\epsilon)$$\n",
        "\n",
        "* Return the mutual information loss `mi`$= -(E_\\text{fix} + E_\\text{mov} - E_\\text{joint})$\n",
        "\n",
        "* Test your implementation with the testcase below. It should yield `tensor(-0.0271)`.\n",
        "\n",
        "**Helpful functions:** `torch.linspace, torch.max, torch.min, torch.exp, torch.pow, torch.sum`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jxeiM0kFTY1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23feece1-d676-4e09-e32c-c44643ac1c21"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-0.0271)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# all tensor are of shape [1, 1, 192, 160, 192].\n",
        "# We convert mask_fix to boolean, so you can use it directly for indexing.\n",
        "def mutual_inf(mask_fix, img_fix, img_mov):\n",
        "    sigma = 0.015\n",
        "    epsilon = 1e-10\n",
        "    mask_fix = mask_fix.to(torch.bool)\n",
        "    \n",
        "    # TODO: draw samples from mask_fix\n",
        "    v_fix = img_fix[mask_fix]\n",
        "    v_mov = img_mov[mask_fix]\n",
        "\n",
        "    # TODO: define bins\n",
        "    with torch.no_grad():\n",
        "        b_fix = torch.linspace(start=torch.min(v_fix), end=torch.max(v_fix), steps=64)\n",
        "        b_mov = torch.linspace(start=torch.min(v_mov), end=torch.max(v_mov), steps=64)\n",
        "\n",
        "    # TODO: estimate histograms\n",
        "    h_fix = torch.exp(-((torch.pow(v_fix.reshape(1,-1) - b_fix.view(-1,1), 2) / (2 * pow(sigma,2)))))\n",
        "    h_mov = torch.exp(-((torch.pow(v_mov.reshape(1,-1) - b_mov.view(-1,1), 2) / (2 * pow(sigma,2)))))\n",
        "    #print(h_fix.shape)\n",
        "    #print(torch.sum(mask_fix))\n",
        "    \n",
        "    # TODO: estimate marginal\n",
        "    p_fix = torch.mean(h_fix, dim=1) / (torch.sum(torch.mean(h_fix,dim=1)) + epsilon)\n",
        "    p_mov = torch.mean(h_mov, dim =1) / (torch.sum(torch.mean(h_mov,dim=1)) + epsilon)\n",
        "    s = torch.matmul(h_fix, torch.transpose(h_mov,1,0))\n",
        "    p_joint = s / (torch.sum(s) + epsilon)\n",
        "    \n",
        "\n",
        "    # TODO: estimate entropies\n",
        "    E_fix = - torch.sum(p_fix * torch.log2(p_fix + epsilon))\n",
        "    E_mov = - torch.sum(p_mov * torch.log2(p_mov + epsilon))\n",
        "    E_joint = - torch.sum(p_joint * torch.log2(p_joint + epsilon))\n",
        "    mi = -(E_fix + E_mov - E_joint)\n",
        "    return mi\n",
        "\n",
        "\n",
        "# testing\n",
        "task12 = torch.load('mdl3_exercise_task12.pth')\n",
        "mutual_inf(mask_fix, task12['img_fix'].float(), img_mov.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW31HDhGTY1D"
      },
      "source": [
        "### Task 2: Correlation layer (35 points)\n",
        "In this task you should compute similarity scores (sum of squared differences) for a set of grid points and a wide range of potential displacements. Your function takes fixed and moving features as well as a mask for the fixed scan as input. The predefined function `get_displacement()` will return a tensor that specifies $9\\times9\\times9=729$ 3D displacements with a maximal range of 20 voxels in each direction.\n",
        "\n",
        "* Define a grid of control points `kpts_fixed` using `F.affine_grid` with `align_corners=True`, a spacing of 12 voxels and range of .925 (to exclude points near the image boundaries). Move it to the same device as `mask_fix`.\n",
        "\n",
        "* Now use `kpts_fixed` with `f.grid_sample` on `mask_fix` to create a down sampled version of it. Use again `align_corners=True` and think of the correct interpolation mode. Cast the result back to boolean.\n",
        "\n",
        "* Next, we want to exclude all key points in `kpts_fixed` that are pointing on the image background. Therefor, view it as $[1, N, 3]$ and index it with `mask_fix_downsampled` also viewed as a 1D tensor.\n",
        "\n",
        "* To store the similarity scores, we create an emtpy tensor `ssd` of shape $[N\\times 9^3]$ on the same device as `feat_fixed`.\n",
        "\n",
        "* Use 32 chunks for unrolling and compute the (dis)similarity as follows:\n",
        "    * get the current subset of grid points `subsampled_kpts_fixed` by indexing `kpts_fixed`with `idx` and view it so all key points are in $D_\\text{out}$ (have a look into `F.grid_sample` for the detailed shape).\n",
        "    * sample the fixed features `feat_patch_fixed` at (the current subset of) grid points and the moving feature tensor `feat_displacements` at the combined (added) coordinates of absolute grid points and relative displacements.\n",
        "   + Square and sum over the channel dimension (C) and save it to the indexed `ssd` tensor.    \n",
        "\n",
        "The final registration is done with the provided function `robust_rigid_fit` (you do not need to call it here), which searches for the most probable correspondences, by filtering out potentially erroneous ones based on their similarity score and the residual of a globally rigid least-square fit.\n",
        "\n",
        "**Helpful notes:**:\n",
        "+ functions: `F.affine_grid, F.grid_sample, view`\n",
        "+ chunk along with view and common indexing, because we will unroll the function call (computed with a loop of several chunks) to save memory."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid_size = [192, 160, 192]\n",
        "num_control_points = [int((size - 1) / 12) + 1 for size in grid_size]\n",
        "num_control_points = torch.tensor(num_control_points)\n",
        "# Generate the grid of control points\n",
        "kpts_fixed = torch.linspace(0.0375, 0.9625, num_control_points[0]).view(-1, 1, 1).repeat(1, num_control_points[1], num_control_points[2])\n",
        "print(kpts_fixed.shape)"
      ],
      "metadata": {
        "id": "Ee0BaYnaEhHo",
        "outputId": "9a06dc8f-36c7-4fb9-a854-d45487358340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 14, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "sDpUNEnETY1E",
        "outputId": "c28f6253-756f-41db-961a-0f8fe3c73b7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[607.7994, 607.7994, 607.7994,  ..., 607.7994, 607.7994, 607.7994],\n",
              "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          ...,\n",
              "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
              "          [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]]),\n",
              " tensor([[[ 4.3167e-01, -6.1667e-01, -8.0167e-01],\n",
              "          [-6.7833e-01, -4.6250e-01, -8.0167e-01],\n",
              "          [ 4.3167e-01, -4.6250e-01, -8.0167e-01],\n",
              "          ...,\n",
              "          [ 4.3167e-01, -2.7567e-08,  9.2500e-01],\n",
              "          [ 5.5500e-01, -2.7567e-08,  9.2500e-01],\n",
              "          [ 3.0833e-01,  1.5417e-01,  9.2500e-01]]]))"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "# Task2 correlation layer\n",
        "def correlation(mask_fix, feat_fixed, feat_moving):\n",
        "\n",
        "    # parameters\n",
        "    grid_step = 12 ### <---\n",
        "    grid_range = 0.925 ### <---\n",
        "    B, _, D, H, W = mask_fix.shape  # -> [1, 1, 192, 160, 192]\n",
        "\n",
        "    # TODO: create grid\n",
        "    theta = torch.eye(3, 4, device=mask_fix.device).unsqueeze(0) * grid_range\n",
        "    kpts_fixed = F.affine_grid(theta, size=(B, 1, D//grid_step, H//grid_step, W//grid_step), align_corners=True)\n",
        "\n",
        "    # TODO: sample from mask_fix\n",
        "    mask_fix_downsampled = F.grid_sample(mask_fix.float(), kpts_fixed, align_corners=True, mode='bilinear').bool()\n",
        "    #print('mask_fix_downsampled.shape', mask_fix_downsampled.shape)\n",
        "    # TODO: exclude all invalid coordinates/grid points\n",
        "    #print('kpts_fixed.shape', kpts_fixed.shape)\n",
        "    kpts_fixed = kpts_fixed.view(1,-1,3)\n",
        "    kpts_fixed = kpts_fixed[:,mask_fix_downsampled.view(-1),:]\n",
        "    #print('kpts_fixed.shape', kpts_fixed.shape)\n",
        "\n",
        "    # TODO: create empty tensor\n",
        "    N = mask_fix_downsampled.sum()\n",
        "    ssd = torch.zeros((N,9**3),device=feat_fixed.device)\n",
        "\n",
        "    unroll_factor = 32\n",
        "    displacements = get_displacement().to(feat_fixed.device).to(feat_fixed.dtype)\n",
        "    for idx in tqdm(torch.tensor_split(torch.arange(B), unroll_factor), disable=True):\n",
        "\n",
        "        # TODO: get subset of grid points\n",
        "        #print('idx', idx)\n",
        "        subsampled_kpts_fixed = kpts_fixed[:,idx,:].view(1,-1,1,1,3)\n",
        "        #print('subsampled_kpts_fixed.shape', subsampled_kpts_fixed.shape) # 1,40,1,1,3\n",
        "\n",
        "        # sample the fixed features feat_patch_fixed at (the current subset of) grid points and\n",
        "        # the moving feature tensor feat_displacements at\n",
        "        # the combined (added) coordinates of absolute grid points and relative displacements.\n",
        "        # Square and sum over the channel dimension (C) and save it to the indexed ssd tensor.\n",
        "        # TODO: sample from features\n",
        "        feat_patch_fixed = F.grid_sample(feat_fixed, subsampled_kpts_fixed, align_corners=True, mode='bilinear')\n",
        "        feat_displacements = F.grid_sample(feat_moving, subsampled_kpts_fixed + displacements, align_corners=True, mode='bilinear')\n",
        "        \n",
        "        # has to be fulfilled\n",
        "        assert list(feat_patch_fixed.shape) == [B, 64, len(idx), 1, 1]\n",
        "        assert list(feat_displacements.shape) == [B, 64, len(idx), 729, 1]\n",
        "        \n",
        "        # TODO: calculate similarity\n",
        "        ssd[idx] = torch.sum((feat_patch_fixed - feat_displacements)**2).squeeze()\n",
        "        #ssd[idx] = torch.sum(diff**2, dim=0)\n",
        "        \n",
        "    return ssd.unsqueeze(0), kpts_fixed\n",
        "\n",
        "# testing\n",
        "correlation(task12['mask_fix'],task12['feat_fix'].to(torch.float),task12['feat_mov'].to(torch.float))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "volTy2T8TY1E"
      },
      "source": [
        "**Optional**: The following code enables to verify your solutions for tasks 1 & 2 improving the mean dice score from ~0.49 to ~0.74:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "e5w0nMzuTY1E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "67f62164-1b6d-4305-bc4d-ccefcbd69542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['feat_fix', 'feat_mov', 'seg_mov', 'mask_fix', 'R', 'seg_fix', 'img_fix', 'img_mov'])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-4c39daf67496>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mimg_mov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask12\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_mov'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#call mutual information before transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutual_inf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_fix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask12\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_fix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_mov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mi before'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-82be91d6a8a0>\u001b[0m in \u001b[0;36mmutual_inf\u001b[0;34m(mask_fix, img_fix, img_mov)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# TODO: estimate histograms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mh_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_fix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb_fix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mh_mov\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_mov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mb_mov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#print(h_fix.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
          ]
        }
      ],
      "source": [
        "#test for task1 and task2\n",
        "\n",
        "task12 = torch.load('mdl3_exercise_task12.pth')\n",
        "\n",
        "print(task12.keys())\n",
        "with torch.no_grad():\n",
        "    mask_fix = task12['mask_fix'].float().cuda()\n",
        "    img_mov = task12['img_mov'].float().cuda()\n",
        "    #call mutual information before transform\n",
        "    loss0 = mutual_inf(mask_fix.cuda(),task12['img_fix'].float().cuda(),img_mov.cuda())\n",
        "    print('mi before',loss0)\n",
        "\n",
        "    with torch.cuda.amp.autocast(enabled=True):\n",
        "            #call your own implementation of correlation layer\n",
        "            feat_fix = task12['feat_fix'].half().cuda()\n",
        "            feat_mov = task12['feat_mov'].half().cuda()\n",
        "            cost,kpts_fixed = correlation(mask_fix,feat_fix,feat_mov)\n",
        "            \n",
        "            #provided function for robust fitting\n",
        "            R = robust_rigid_fit(cost.cuda(),kpts_fixed.cuda(),feat_fix)\n",
        "            \n",
        "    #mutual information requires 32bit precision\n",
        "    grid = F.affine_grid(R, (1,1,D,H,W))\n",
        "    img_warped = F.grid_sample(img_mov,grid.float(),mode='bilinear')\n",
        "    loss1 = mutual_inf(mask_fix,task12['img_fix'].float().cuda(),img_warped)\n",
        "    print('mi after',loss1)  \n",
        "    seg_fix = task12['seg_fix'].float().cuda()\n",
        "    seg_mov = task12['seg_mov'].float().cuda()\n",
        "\n",
        "    seg_mov_warped = F.grid_sample(seg_mov.float(), grid, mode='nearest')\n",
        "\n",
        "    d0 = dice_coeff(seg_fix.cpu(),seg_mov.cpu(),5) \n",
        "    print(d0)\n",
        "    print('mean dice before: ',d0.mean().item())\n",
        "    d1 = dice_coeff(seg_fix.cpu(),seg_mov_warped.cpu(),5)\n",
        "    print(d1)\n",
        "    print('mean dice after: ', d1.mean().item())\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_-O0svETY1E"
      },
      "source": [
        "### Task 3: Define network and train CNNs with mutual information (40 points)\n",
        "To extract suitable features for registration both scans should be fed into a 3D CNN with trainable parameters. To  account for the differences between CT and MRI, but also encourage the learning of multi-modal relationships you should build your network based on three modules: one for CT, one for MRI (each with two blocks) and one that is shared for both (with three blocks).\n",
        "\n",
        "#### CIR block\n",
        "First, let's build a `CIR` function that implement two convolutional building block and simply return them as a `nn.Sequential`. A convolutional building block consists of\n",
        "+ `nn.Conv3d` with `kernel_size=3` and padding mode 'same'.\n",
        "+ `nn.InstanceNorm3d`\n",
        "+ `nn.LeakyReLU`\n",
        "\n",
        "The first convolution should get the option to have a `stride=2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG1_YZOnTY1F"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "def CIR(in_channels:int , out_channels:int, stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv3d(in_channels=in_channels,out_channels=out_channels, kernel_size=2,stride=2,padding=8),\n",
        "        nn.InstanceNorm3d(out_channels),\n",
        "        nn.LeakyReLU(),\n",
        "\n",
        "        nn.Conv3d(in_channels=in_channels,out_channels=out_channels, kernel_size=3,stride=stride,padding=8),\n",
        "        nn.InstanceNorm3d(out_channels),\n",
        "        nn.LeakyReLU()\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_vX44rVTY1F"
      },
      "source": [
        "#### ModalityNet\n",
        "The modality specific modules should both receive a single-channel input and start with `base=16` feature maps that are doubled to 32 in the second `CIR` block. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDFTGQq8TY1F"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "class ModalityNet(nn.Module):\n",
        "    def __init__(self, base):\n",
        "        super(ModalityNet, self).__init__()\n",
        "        self.base_channels = 16\n",
        "        self.cir1 = CIR(self.base_channels,16)\n",
        "        self.cir2 = CIR(16, 32)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cir1(x)\n",
        "        x = self.cir2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfxaA8SrTY1F"
      },
      "source": [
        "#### SharedNet\n",
        "The shared subnetwork should receive the 32-channel feature maps and comprise three blocks of the same pattern as above: doubling of channels and `stride=2` in second block. The last block keeps the channel dimension and has no stride. This yields 64-channel feature that will be mapped into a range of 0 to 1 using a `nn.Sigmoid`. These feature tensors (for fixed = MRI and moving = CT) will be the input of your correlation layer from Task 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLaHawNITY1F"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "class SharedNet(nn.Module):\n",
        "    def __init__(self, base, out_channels):\n",
        "        super(SharedNet, self).__init__()\n",
        "\n",
        "        self.cir1 = CIR(in_channels=32, out_channels=32)\n",
        "        self.cir2=CIR(32,64,stride=2)\n",
        "        self.cir3=CIR(64,64,stride=0)\n",
        "        self.sig=nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x= self.cir1(x)\n",
        "        x=self.cir2(x)\n",
        "        x=self.cir3(x)\n",
        "        x=self.sig(x)\n",
        "\n",
        "        return x   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT6qg8ZJTY1F"
      },
      "source": [
        "#### FeatureNet\n",
        "\n",
        "The `FeatureNet` joins the two individual `ModalityNet` and the `SharedNet`. The last one will be - as its name suggests - shared between the two modalities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-kSQA0BTY1G"
      },
      "outputs": [],
      "source": [
        "class FeatureNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureNet, self).__init__()\n",
        "        \n",
        "        base = 16\n",
        "        out_channels = 64\n",
        "        \n",
        "        self.modality1_net = ModalityNet(base)\n",
        "        self.modality2_net = ModalityNet(base)\n",
        "        self.shared_net = SharedNet(base, out_channels)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = self.modality1_net(x)\n",
        "        y = self.modality2_net(y)\n",
        "        x = self.shared_net(x)\n",
        "        y = self.shared_net(y)\n",
        "        return x, y\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKkuNkxGSpYh"
      },
      "source": [
        "During training, the resulting SSD tensor is again fed into robust_rigid_fit, which return a 1x3x4 matrix to obtain a deformation grid that will be applied to the moving scan. Afterwards the mutual information can be computed and minimised as loss. The training loop is pre-defined, we use a batch-size of 1 (hence InstanceNorm) and train for 30 epochs (240 iterations).\n",
        "\n",
        "During training, you see visualisations of the estimated transforms (using it to warp the moving\n",
        "segmentation) and after training you can run the provided evaluation functions and see that the Dice overlap should increase from 43% to above 60%.\n",
        "\n",
        "**Optional**: Plot the initial dice values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnBbOXVqACSZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "get_dice_all(TRAIN_CASES,segs_fix_train,segs_mov_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcgGmCS5ACSc"
      },
      "outputs": [],
      "source": [
        "# training loop \n",
        "\n",
        "num_epochs = 30\n",
        "init_lr = 0.001\n",
        "device = 'cuda'\n",
        "\n",
        "net = FeatureNet().to(device)\n",
        "parameter_count(net)\n",
        "optimizer = optim.Adam(net.parameters(), lr=init_lr)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "losses = torch.zeros(num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.time()\n",
        "    running_loss = 0\n",
        "    rand_idx = torch.randperm(len(TRAIN_CASES))\n",
        "    for idx in rand_idx:\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        rand_idx1 = torch.randint(8, (1,))[0]\n",
        "        img_fix = imgs_fix_train[idx:idx+1, rand_idx1:rand_idx1+1].to(device,non_blocking=True)# + 1\n",
        "        img_mov = imgs_mov_train[idx:idx+1].to(device,non_blocking=True)\n",
        "        seg_fix = segs_fix_train[idx:idx+1, rand_idx1:rand_idx1+1].long().to(device,non_blocking=True)\n",
        "        seg_mov = segs_mov_train[idx:idx+1].to(device,non_blocking=True).long()\n",
        "        mask_fix = masks_fix_train[idx:idx+1, rand_idx1:rand_idx1+1].to(device,non_blocking=True)\n",
        "        mask_mov = masks_mov_train[idx:idx+1].to(device,non_blocking=True)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled=True):\n",
        "\n",
        "            #TODO: call your own implementation of network architecture and correlation layer\n",
        "            feat_fix, feat_mov =\n",
        "            cost,kpts_fixed =\n",
        "            \n",
        "            #provided function for robust fitting\n",
        "            R = robust_rigid_fit(cost,kpts_fixed,feat_fix)\n",
        "            \n",
        "        #mutual information requires 32bit precision\n",
        "        with torch.cuda.amp.autocast(enabled=False): \n",
        "            grid = F.affine_grid(R, (1,1,D,H,W))\n",
        "            img_warped = F.grid_sample(img_mov,grid.float(),mode='bilinear')\n",
        "            \n",
        "            #TODO: call your own implementation for mutual information loss\n",
        "            loss =\n",
        "\n",
        "        seg_mov_warped = F.grid_sample(F.one_hot(seg_mov, 5).view(1, D, H, W, -1).permute(0, 4, 1, 2, 3).float(), grid.float(), mode='bilinear').argmax(1)\n",
        "        if(rand_idx1==4):\n",
        "            plt.figure()\n",
        "            q100 = float(torch.topk(img_fix[0,0,:,60,:].reshape(-1),100)[0].cpu().data[-1:])\n",
        "            gray1 = torch.clamp(img_fix[0,0,:,60,:].data.cpu().t().flip([0,1]),0,q100)/q100\n",
        "            rgb = overlaySegment(gray1,seg_mov_warped[0,:,60,:].long().data.cpu().t().flip([0,1]))\n",
        "            plt.imshow(rgb)\n",
        "            plt.show()\n",
        "            \n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    running_loss /= len(TRAIN_CASES)\n",
        "    losses[epoch] = running_loss\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    print('epoch (train): {:02d} -- loss: {:.3f} -- time(s): {:.1f}'.format(epoch, running_loss, t1-t0))\n",
        "    gpu_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djw5HRrgVggF"
      },
      "source": [
        "Plot the loss and store network weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzjUC2llACSc"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses)\n",
        "FOLD = 3\n",
        "torch.save(net.cpu().state_dict(), 'net_mi_fold{}.pth'.format(FOLD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYw4rDbgVnYy"
      },
      "source": [
        "**Evaluation**: Run the following code for a final evaluation. You dice should increase from 43% to above 60%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcC4sIb3ACSd"
      },
      "outputs": [],
      "source": [
        "#quantitaive evaluation of the trained network, should return around 60% Dice after registration\n",
        "net = FeatureNet().to(device)\n",
        "net.load_state_dict(torch.load('net_mi_fold{}.pth'.format(FOLD)))\n",
        "net.eval()\n",
        "parameter_count(net)\n",
        "\n",
        "torch.manual_seed(30)\n",
        "TEST_CASES = TRAIN_CASES\n",
        "with torch.no_grad():\n",
        "    dice_init_all = torch.zeros(4,len(TEST_CASES),4)\n",
        "    dice_all = torch.zeros(4,len(TEST_CASES),4)\n",
        "    for i in range(4):\n",
        "        for j, case in enumerate(TEST_CASES):\n",
        "            img_fix, img_mov, seg_fix, seg_mov, mask_fix, mask_mov = load_case(case)\n",
        "\n",
        "            img_fix = img_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)# + 1\n",
        "            img_mov = img_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "            seg_fix = seg_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "            seg_mov = seg_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "            mask_fix = mask_fix.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "            mask_mov = mask_mov.to(device,non_blocking=True).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "            R = generate_random_rigid_3d()\n",
        "            grid = F.affine_grid(R[:3,:4].unsqueeze(0), (1,1,D,H,W))\n",
        "            img_fix_ = F.grid_sample(img_fix, grid)\n",
        "            seg_fix_ = F.grid_sample(F.one_hot(seg_fix[0, 0]).permute(3, 0, 1, 2).unsqueeze(0).float(), grid).argmax(1, keepdim=True)\n",
        "            mask_fix_ = (F.grid_sample(mask_fix.float(), grid)>0.5).float()\n",
        "            with torch.cuda.amp.autocast():\n",
        "                feat_fix, feat_mov = net(img_fix_.contiguous(), img_mov.contiguous())\n",
        "                ssd_cost,kpts_fix = correlation(mask_fix_,feat_fix,feat_mov)\n",
        "            R = robust_rigid_fit(ssd_cost,kpts_fix,feat_fix)\n",
        "            \n",
        "            grid = F.affine_grid(R, (1,1,D,H,W))\n",
        "            seg_mov_warped = F.grid_sample(seg_mov.float(), grid, mode='nearest')\n",
        "\n",
        "            d = dice_coeff(seg_fix.cpu(),seg_mov.cpu(),5); print(d,d.mean())\n",
        "            d0 = dice_coeff(seg_fix_.cpu(),seg_mov.cpu(),5); print(d0,d0.mean())\n",
        "            d1 = dice_coeff(seg_fix_.cpu(),seg_mov_warped.cpu(),5); print(d1,d1.mean())\n",
        "            print()\n",
        "\n",
        "            dice_init_all[i, j] = d0\n",
        "            dice_all[i, j] = d1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BR13t48ACSe"
      },
      "outputs": [],
      "source": [
        "# Print dice \n",
        "\n",
        "print('Initial dice: ', (dice_init_all.sum()/(dice_init_all>0).sum()).item())\n",
        "print('Dice after reg:', (dice_all.sum()/(dice_init_all>0).sum()).item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus Task: Deformable Image Registration"
      ],
      "metadata": {
        "collapsed": false,
        "id": "0z4T6NR5TY1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deformable image registration is the process of finding correspondence between images that are not linked by simple rigid shifts and rotations. It is more realistic setup, because patients have many degrees of freedom and can move and deform due to many processes including simply being lying in a slightly different position from day to day, weight loss, tumor shrinkage, normal tissue shrinkage, inflammation of normal tissue, and motion due to respiration.\n",
        "\n",
        "In the bonus task we will use method proposed in [Label-driven weakly-supervised learning for multimodal deformable image registration](https://arxiv.org/pdf/1711.01666.pdf) paper to register CT images taken at inspiration and expiration respiratory phases of the same patient."
      ],
      "metadata": {
        "collapsed": false,
        "id": "50qe6ZWvTY1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can notice that in the provided template we are using several methods from [MONAI](https://docs.monai.io/en/stable/). MONAI is a PyTorch-based, open-source framework for deep learning in healthcare imaging. It contains variety of very useful tools that can save a lot of time during development of deep-learning based solution for medical image processing. You are more than welcome to explore it."
      ],
      "metadata": {
        "collapsed": false,
        "id": "lLa90yWMTY1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Environment"
      ],
      "metadata": {
        "collapsed": false,
        "id": "7xGsDbpeTY1I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import required packages:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "UPNM6HScTY1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torchinfo\n",
        "from torch.nn import MSELoss\n",
        "\n",
        "from monai.apps import download_and_extract\n",
        "\n",
        "from monai.networks.blocks import Warp\n",
        "\n",
        "from monai.losses import DiceLoss, BendingEnergyLoss\n",
        "from monai.metrics import DiceMetric\n",
        "\n",
        "from monai.data import DataLoader, Dataset\n",
        "from monai.transforms import Compose, LoadImaged, Resized, ScaleIntensityRanged\n",
        "from monai.utils import first"
      ],
      "metadata": {
        "id": "vDRrRAXoTY1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data"
      ],
      "metadata": {
        "collapsed": false,
        "id": "nNITpXoXTY1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and extract the dataset:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "cKypLNHiTY1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "resource = \"https://zenodo.org/record/3835682/files/training.zip\"\n",
        "\n",
        "compressed_file = \"paired_ct_lung.zip\"\n",
        "data_dir = \"paired_ct_lung\"\n",
        "if not os.path.exists(os.path.join('.', data_dir)):\n",
        "    download_and_extract(resource, compressed_file)\n",
        "    os.rename(os.path.join('.', \"training\"), data_dir)"
      ],
      "metadata": {
        "id": "LUuRMRHgTY1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training and validation data dictionaries:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "xZcOqpGLTY1K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "data_dicts = [\n",
        "    {\n",
        "        \"fixed_image\": os.path.join(data_dir, \"scans/case_%03d_exp.nii.gz\" % idx),\n",
        "        \"moving_image\": os.path.join(data_dir, \"scans/case_%03d_insp.nii.gz\" % idx),\n",
        "        \"fixed_label\": os.path.join(data_dir, \"lungMasks/case_%03d_exp.nii.gz\" % idx),\n",
        "        \"moving_label\": os.path.join(data_dir, \"lungMasks/case_%03d_insp.nii.gz\" % idx),\n",
        "    }\n",
        "    for idx in range(1, 21)\n",
        "]\n",
        "\n",
        "train_files, val_files = data_dicts[:18], data_dicts[18:]"
      ],
      "metadata": {
        "id": "5BkB2xV7TY1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define data processing pipeline using MONAI transforms:\n",
        "- LoadImaged: loads the lung CT images and labels from NIfTI format files, \"ensure_channel_first=True\" ensure that the first dim is channel.\n",
        "- ScaleIntensityRanged: extracts intensity range [-285, 3770] and scales to [0, 1].\n",
        "- Resized: resize images to the same size."
      ],
      "metadata": {
        "collapsed": false,
        "id": "0vf1z8OoTY1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"fixed_image\", \"moving_image\", \"fixed_label\", \"moving_label\"], ensure_channel_first=True),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"fixed_image\", \"moving_image\"],\n",
        "            a_min=-285,\n",
        "            a_max=3770,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        Resized(\n",
        "            keys=[\"fixed_image\", \"moving_image\", \"fixed_label\", \"moving_label\"],\n",
        "            mode=(\"trilinear\", \"trilinear\", \"nearest\", \"nearest\"),\n",
        "            align_corners=(True, True, None, None),\n",
        "            spatial_size=(96, 96, 104),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        LoadImaged(keys=[\"fixed_image\", \"moving_image\", \"fixed_label\", \"moving_label\"], ensure_channel_first=True),\n",
        "        ScaleIntensityRanged(\n",
        "            keys=[\"fixed_image\", \"moving_image\"],\n",
        "            a_min=-285,\n",
        "            a_max=3770,\n",
        "            b_min=0.0,\n",
        "            b_max=1.0,\n",
        "            clip=True,\n",
        "        ),\n",
        "        Resized(\n",
        "            keys=[\"fixed_image\", \"moving_image\", \"fixed_label\", \"moving_label\"],\n",
        "            mode=(\"trilinear\", \"trilinear\", \"nearest\", \"nearest\"),\n",
        "            align_corners=(True, True, None, None),\n",
        "            spatial_size=(96, 96, 104),\n",
        "        ),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "AY9KH2BVTY1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show example of inspiration and expiration respiratory phases of the same patient:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "hCnaDymGTY1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "check_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "check_loader = DataLoader(check_ds, batch_size=1)\n",
        "check_data = first(check_loader)\n",
        "fixed_image = check_data[\"fixed_image\"][0][0].permute(1, 0, 2)\n",
        "fixed_label = check_data[\"fixed_label\"][0][0].permute(1, 0, 2)\n",
        "moving_image = check_data[\"moving_image\"][0][0].permute(1, 0, 2)\n",
        "moving_label = check_data[\"moving_label\"][0][0].permute(1, 0, 2)\n",
        "\n",
        "print(f\"moving_image shape: {moving_image.shape}, \" f\"moving_label shape: {moving_label.shape}\")\n",
        "print(f\"fixed_image shape: {fixed_image.shape}, \" f\"fixed_label shape: {fixed_label.shape}\")\n",
        "\n",
        "plt.figure(\"check\", (12, 6))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.title(\"moving_image\")\n",
        "plt.imshow(moving_image[:, :, 50], cmap=\"gray\")\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.title(\"moving_label\")\n",
        "plt.imshow(moving_label[:, :, 50])\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.title(\"fixed_image\")\n",
        "plt.imshow(fixed_image[:, :, 50], cmap=\"gray\")\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.title(\"fixed_label\")\n",
        "plt.imshow(fixed_label[:, :, 50])\n",
        "\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7Dh7AWUWTY1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create training and validation data loaders:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "dP5Gg7UFTY1M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
        "\n",
        "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=1, num_workers=4)"
      ],
      "metadata": {
        "id": "G0ULqIJ_TY1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ryk5it-1TY1M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A neural network takes as an input concatenation of moving and fixed images on channel dimension and computes a dense displacement field (DDF). Moving image warped with the DDF should correspond to fixed image. We will refer to this neural network as \"LocalNet\" because it computes local non-rigid deformations."
      ],
      "metadata": {
        "collapsed": false,
        "id": "Bccw_mrhTY1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LocalNet designed as a 3D convolutional neural network with three residual blocks.\n",
        "\n",
        "Each residual block consists of three convolution groups:\n",
        "- a `nn.Conv3d` with same padding.\n",
        "- a `nn.InstanceNorm3d` for normalisation.\n",
        "- a `nn.LeakyReLU` as non-linearity.\n",
        "\n",
        "Residual connection is branched out after first convolution group and summed up with output of normalization layer of third convolution group.\n",
        "Number of channels is changed by first convolution group and remains the same after that.\n",
        "Kernel size is the same for all convolution groups inside the residual block.\n",
        "Output of third convolution group downsampled by factor 2 using`nn.MaxPool3d` .\n",
        "\n",
        "Implement residual block:\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "LeyKaHyhTY1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "\n",
        "class ResidualEncoderBlock(nn.Module):\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "pURGy0vMTY1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First residual block increases number of channels to 32, second - to 64, and third - to 128.\n",
        "Kernel size of first residual block is equal to 7, while for second and third residual blocks kernel size are equal to 3.\n",
        "Output of third residual block is processed by addition convolution group:\n",
        "- a `nn.Conv3d` with number of output channels equal to 256, kernel size equal to 3 and same padding.\n",
        "- a `nn.InstanceNorm3d` for normalisation.\n",
        "- a `nn.LeakyReLU` as non-linearity.\n",
        "\n",
        "Output of convolution group is processed by another convolution layer (with kernel size 3 and same padding) that reduces number of channels to 3 and computes coarse dense displacement map.\n",
        "Finally, coarse displacement map is interpolated to match dimensions of moving image and get displacement for its each voxel.\n",
        "\n",
        "Implement LocalNet:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "TH8bzg5_TY1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class LocalNet(nn.Module):\n",
        "\n",
        "    # TODO\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_mQObZ5LTY1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate LocalNet and print its summary.\n",
        "The number of trainable parameters should be equal to 3,013,219."
      ],
      "metadata": {
        "collapsed": false,
        "id": "TCAnHV9GTY1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "model = LocalNet()\n",
        "torchinfo.summary(model, (1, 2, 96, 96, 104))"
      ],
      "metadata": {
        "id": "y9yFHJ2STY1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training loop"
      ],
      "metadata": {
        "collapsed": false,
        "id": "nqUGtrpQTY1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the training process we can use loss function that measures the correspondence based on:\n",
        "- voxel-level labels - more intuitive way, it can be implemented, for example, using MSE loss between warped moving image and fixed image.\n",
        "- anatomy-level labels - it can be implemented, for example, using Dice loss between warped label of moving image and label of fixed image.\n",
        "\n",
        "You will compare both of these loss functions."
      ],
      "metadata": {
        "collapsed": false,
        "id": "4ChAaOqCTY1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below you can find the template of the training loop method.\n",
        "\n",
        "Pay attention that we are using again handy methods from MONAI library: warp layer, dice loss function and dice metric. You can read more about the usage in [documentation](https://docs.monai.io/en/stable/) of MONAI library.\n",
        "\n",
        "Complete missing parts of the training loop. You can use forward method, adam optimizer and dice metric defined below in your implementation:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "jm2x4kFjTY1O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_model(loss_function, anatomy_loss=False, max_epochs = 30):\n",
        "\n",
        "    device = torch.device(\"cuda:0\")\n",
        "\n",
        "    model = LocalNet().to(device)\n",
        "    warp_layer = Warp().to(device)\n",
        "    def forward(batch_data, model):\n",
        "        fixed_image = batch_data[\"fixed_image\"].to(device)\n",
        "        moving_image = batch_data[\"moving_image\"].to(device)\n",
        "        moving_label = batch_data[\"moving_label\"].to(device)\n",
        "\n",
        "        # predict DDF through LocalNet\n",
        "        ddf = model(torch.cat((moving_image, fixed_image), dim=1))\n",
        "\n",
        "        # warp moving image and label with the predicted ddf\n",
        "        pred_image = warp_layer(moving_image, ddf)\n",
        "        pred_label = warp_layer(moving_label, ddf)\n",
        "\n",
        "        return ddf, pred_image, pred_label\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), 1e-5)\n",
        "    dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
        "    regularization = BendingEnergyLoss()\n",
        "\n",
        "    best_metric = 0\n",
        "    epoch_loss_values = []\n",
        "    metric_values = []\n",
        "    for epoch in range(max_epochs):\n",
        "\n",
        "        model.train()\n",
        "        for batch_data in train_loader:\n",
        "\n",
        "            # TODO: implement training for current epoch append loss to loss values\n",
        "\n",
        "            pred_label =\n",
        "            pred_image =\n",
        "\n",
        "            if anatomy_loss:\n",
        "                loss = loss_function(pred_label, fixed_label)\n",
        "            else:\n",
        "                loss = loss_function(pred_image, fixed_image)  + 10 * regularization(ddf)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for val_data in val_loader:\n",
        "\n",
        "                # TODO: complete validation and append dice to metric values\n",
        "\n",
        "    return epoch_loss_values, metric_values"
      ],
      "metadata": {
        "id": "XVepwoqUTY1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model with voxel-level loss:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "qNx4uwv1TY1O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "voxel_loss = MSELoss()\n",
        "epoch_values_with_voxel_loss, metric_values_with_voxel_loss = train_model(voxel_loss, anatomy_loss=False)"
      ],
      "metadata": {
        "id": "u1eLVtxKTY1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model with anatomy-level loss:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "YkglxZQlTY1O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "anatomy_loss = DiceLoss()\n",
        "epoch_values_with_anatomy_loss, metric_values_with_anatomy_loss = train_model(anatomy_loss, anatomy_loss=True)"
      ],
      "metadata": {
        "id": "AhWtamETTY1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voxel-level labels for correspondence learning are impossible to reliably obtain from medical image data.\n",
        "In contrast, loss function computed on anatomy-level labels enforce model to learn high level semantic correspondence between images that is much easier to the model to understand during the training.\n",
        "\n",
        "You should notice the difference:"
      ],
      "metadata": {
        "collapsed": false,
        "id": "gpYXo16DTY1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(\"train\", (12, 6))\n",
        "plt.title(\"Val Mean Dice\")\n",
        "x = [(i + 1) for i in range(len(metric_values_with_anatomy_loss))]\n",
        "y_1 = metric_values_with_anatomy_loss\n",
        "y_2 = metric_values_with_voxel_loss\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.plot(x, y_1, label='dice_for_anatomy_loss')\n",
        "plt.plot(x, y_2, label='dice_for_voxel_loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sa73gsV2TY1P"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "af50f4e22f104909961c5e984ec8ae44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa876b63714f47c09493aea69a6d3142",
              "IPY_MODEL_34a0267b37564177b588fd44896d6536",
              "IPY_MODEL_a1912fd90ec24643b6ea4fbf923730b0"
            ],
            "layout": "IPY_MODEL_72218cd3c0aa49db91b05fe5e4a2c59c"
          }
        },
        "fa876b63714f47c09493aea69a6d3142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4675427763a4ddb9e03fe2bd73dada8",
            "placeholder": "​",
            "style": "IPY_MODEL_f6c870acd5ee416f828e98cc9a188344",
            "value": "load cases: 100%"
          }
        },
        "34a0267b37564177b588fd44896d6536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29cd38b3117442bea3f49cfb169779f3",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eceb3c02dabc46aba25158f5d1e47f45",
            "value": 8
          }
        },
        "a1912fd90ec24643b6ea4fbf923730b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_371da326217f4eeabf89f46cabf26ca4",
            "placeholder": "​",
            "style": "IPY_MODEL_2494c23ec905440cb23cd56d09329cdc",
            "value": " 8/8 [00:34&lt;00:00,  3.50s/it]"
          }
        },
        "72218cd3c0aa49db91b05fe5e4a2c59c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4675427763a4ddb9e03fe2bd73dada8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6c870acd5ee416f828e98cc9a188344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29cd38b3117442bea3f49cfb169779f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eceb3c02dabc46aba25158f5d1e47f45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "371da326217f4eeabf89f46cabf26ca4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2494c23ec905440cb23cd56d09329cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}